\begin{abstract}

\noindent 

LSTM models have become increasingly popular for
the task of language modeling, mostly
because of their capability to capture long-distance
dependencies. 
Dependencies in natural language are often sensitive
to syntactic structure.
Capturing such dependencies is challenging 
since LSTM models 
do not have explicit
structural representations.
In this paper we focus on noun-verb number agreement
as an example of a syntactic dependency.
%
We investigate the sensitivity of
LSTM language models to
statistic, syntactic and semantic information
when predicting the number of the next verb.
Our results show that these models are able
to learn the numbers for most nouns and verbs
using statistic regularities.
We also found evidence for a modest sensitivity
to syntactic structure exposed by function words. 
\end{abstract}

% main idea
%- language model, number agreement

% key findings
%- able to learn the numbers of nouns and verbs
%- modest sensitivity to syntactic structure

