{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Grammar in Neural language models\n",
    "\n",
    "We will investigate a language model trained on the penn treebank dataset, using the code provided at https://github.com/pytorch/examples/tree/master/word_language_model. The model consists of an encoder with 2 hidden LSTM layers with 1500 units, and a linear output layer to which a softmax function is applied. The word embeddings have dimensionality 1500. The model is trained for 40 epochs with a dropout factor of 0.65 and has a test perplexity of 72.30 on the test set. If you are interested in more detail in the model, we advise you to look at the repository containing the code.\n",
    "\n",
    "In this notebook, we will walk you through an example of how you can compute the probabilties of the next word in a sentence. You can then use this to start your replication of Linzen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2834)\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you downloaded and extracted the zipfile, you should have all data required: the model, and the pickled dictionary mapping words to indices.\n",
    "\n",
    "Lets start by loading the model. Because the model was trained on a GPU, we need to specifically say that it should be loaded on the CPU when we load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel (\n",
      "  (drop): Dropout (p = 0.65)\n",
      "  (encoder): Embedding(10000, 1500)\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
      "  (decoder): Linear (1500 -> 10000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the architecture of your model\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a single sentence\n",
    "\n",
    "We will give an example of how you can get the probabilties for the next word in a single sentence. We will uset he example sentence:<br>\n",
    "\n",
    "\"This is a sentence with seven\"\n",
    "\n",
    "And print the probabilities of completing this sentence with either 'words', 'characters', 'thursday', 'days' or 'walk'. As the model itself does not include the mapping from words to indices, we will need to do this as a preprocessing step. The dictionary that maps words to indices is stored in a pickled file called 'dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that can be used to evaluate a single sentence and print the probabilities of finishing this sentence with a word from a list of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    #print (sentence, '\\n')\n",
    "    #print ('\\n'.join(\n",
    "    #        ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "    #        ) )\n",
    "\n",
    "    return {word : get_prob(word) for word in check_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test sentence and words to check\n",
    "test_sentence = 'this is a sentence with seven'\n",
    "check_words = ['words', 'characters', 'thursday', 'days', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': 7.000279583735391e-05,\n",
       " 'days': 0.002636461518704891,\n",
       " 'thursday': 1.165580556516943e-06,\n",
       " 'walk': 5.386583779909415e-06,\n",
       " 'words': 0.0009886504849418998}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it yourself\n",
    "\n",
    "You can now start with your replication of Linzen's paper, for which the first step is to try different inputs with varying distances etc. to get a feeling for what the model is doing and to familiarise yourself with using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are': 0.031047170981764793, 'is': 0.00332761462777853}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"the toy on the tables\"\n",
    "check_words = [\"is\", \"are\"]\n",
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentence, options, correct-option):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the product of the company', ['looks', 'look'], 'looks'),\n",
       " ('the products of the company', ['looks', 'look'], 'look'),\n",
       " ('the product of the companies', ['looks', 'look'], 'looks'),\n",
       " ('the products of the companies', ['looks', 'look'], 'look'),\n",
       " ('the product that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the products that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the product that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the products that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the product that the company produces', ['looks', 'look'], 'looks'),\n",
       " ('the products that the company produces', ['looks', 'look'], 'look'),\n",
       " ('the product that the companies produce', ['looks', 'look'], 'looks'),\n",
       " ('the products that the companies produce', ['looks', 'look'], 'look')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose sentence prefixes with frequent words.\n",
    "# The sentence prefixes are intended to test intervening nouns.\n",
    "\n",
    "\n",
    "NN = ['company', 'year', 'market', 'share', 'stock', 'system', 'president', 'business', \n",
    "      'quarter', 'government', 'time', 'week', 'price', 'group', 'interest',\n",
    "      'industry', 'unit','month', 'rate', 'investment', 'state', 'producer', 'income', \n",
    "      'program', 'bank', 'part', 'plan', 'sale', 'issue', 'tax', 'way', 'loss', 'executive', 'day', 'bid', 'data', 'line','hour', 'plant', 'concern']\n",
    "\n",
    "NNS = ['companies', 'years', 'markets', 'shares', 'stocks', 'systems', 'presidents', \n",
    "       'businesses', 'quarters', 'governments', 'times', 'weeks', 'prices', 'groups', 'interests', 'industries', \n",
    "       'units', 'months', 'rates', 'investments', 'states', 'producers', 'incomes', 'programs', 'banks', 'parts', 'plans', \n",
    "      'sales', 'issues', 'taxes', 'ways', 'losses', 'executives', 'days', 'bids', 'data', 'lines', 'hours', 'plants', 'concerns',]\n",
    "\n",
    "VBP = ['are', 'have', 'do', 'say', 'think', 'want', 'expect', 'include', 'ask', \n",
    "       'make', 'need', 'know', 'see', 'get', 'seem', 'remain', 'continue', 'show', 'buy', \n",
    "       'feel', 'go', 'sell', 'take', 'use', 'plan', 'look', 'tend', 'hope', 'argue', 'give',\n",
    "       'pay', 'appear', 'suggest', 'fear', 'find', 'come', 'offer', 'contend', 'agree', 'provide']\n",
    "\n",
    "VBZ = ['is', 'has', 'does', 'says', 'thinks', 'wants', 'expects', 'includes', 'asks', 'makes',\n",
    "      'needs', 'knows', 'sees', 'gets', 'seems', 'remains', 'continues', 'shows', 'buys', 'feels', 'goes', 'sells',\n",
    "      'takes', 'uses', 'plans', 'looks', 'tends', 'hopes', 'argues', 'gives', 'pays', 'appears', 'suggests', 'fears',\n",
    "      'finds', 'comes', 'offers', 'contends', 'agrees', 'provides']\n",
    "\n",
    "attractor_helpers = ['in the', 'by the', 'close to the', 'of the', 'at the', 'ant not the', 'without']\n",
    "\n",
    "words = {\n",
    "    \"NN1\" : \"product\",\n",
    "    \"NNS1\" : \"products\",\n",
    "    \"NN2\" : \"company\",\n",
    "    \"NNS2\" : \"companies\",\n",
    "    \"VBP1\" : \"looks\",\n",
    "    \"VBZ1\" : \"look\",\n",
    "    \"VBP2\" : \"produces\",\n",
    "    \"VBZ2\" : \"produce\",\n",
    "}\n",
    "sentences = [\n",
    "    (f\"the {words['NN1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1'])\n",
    "]\n",
    "\n",
    "print(\"(sentence, options, correct-option):\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_rate(sentences):\n",
    "    result = [1 if is_correct_prediction(s[0], s[1], s[2]) else 0 for s in sentences]\n",
    "    #print(result)\n",
    "    return 1- sum(result)/len(result)\n",
    "    \n",
    "\n",
    "def is_correct_prediction(sentence, check_words, correct_word):\n",
    "    predictions = evaluate(lm, dictionary, sentence, check_words)\n",
    "    predicted_word = max(predictions, key=predictions.get)\n",
    "    return predicted_word == correct_word\n",
    "    \n",
    "is_correct_prediction(sentences[0][0], sentences[0][1], sentences[0][2])\n",
    "calculate_error_rate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with one and without attractors\n",
    "\n",
    "def gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices_x = []\n",
    "    indices_u = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y = np.random.randint(num_words, size=2)\n",
    "            if (x,y) not in indices_x:\n",
    "                indices_x.append((x,y))\n",
    "                break\n",
    "        while True:\n",
    "            u,v = np.random.randint(num_words, size=2)\n",
    "            if (u,v) not in indices_u:\n",
    "                indices_u.append((u,v))\n",
    "                break\n",
    "        sentences_si.append((f\"the {NN[x]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "        sentences_pl.append((f\"the {NNS[u]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "def gen_one_attractor(num_sentences, num_words,same,NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices_x = []\n",
    "    indices_u = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y,z = np.random.randint(num_words, size=3)\n",
    "            if (x,y,z) not in indices_x:\n",
    "                indices_x.append((x,y,z))\n",
    "                break\n",
    "        while True:\n",
    "            u,v,w = np.random.randint(num_words, size=3)\n",
    "            if (u,v,w) not in indices_u:\n",
    "                indices_u.append((u,v,w))\n",
    "                break\n",
    "        if(same):\n",
    "            sentences_si.append((f\"the {NN[x]} of the {NN[z]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((f\"the {NNS[u]} of the {NNS[w]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "        else:\n",
    "            sentences_si.append((f\"the {NN[x]} of the {NNS[z]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((f\"the {NNS[u]} of the {NN[w]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "            \n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "num_sentences = 1000\n",
    "num_words = len(NN)\n",
    "\n",
    "no_attractors_si, no_attractors_pl = gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_same, one_attractor_pl_same = gen_one_attractor(num_sentences, num_words,True, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_diff, one_attractor_pl_diff = gen_one_attractor(num_sentences, num_words,False, NN, NNS, VBP, VBZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the producer', ['plan', 'plans'], 'plans'), ('the unit', ['offer', 'offers'], 'offers')]\n",
      "[('the industries', ['are', 'is'], 'are'), ('the incomes', ['suggest', 'suggests'], 'suggest')]\n",
      "[('the interest of the tax', ['buy', 'buys'], 'buys'), ('the president of the government', ['sell', 'sells'], 'sells')]\n",
      "[('the units of the data', ['hope', 'hopes'], 'hope'), ('the weeks of the weeks', ['continue', 'continues'], 'continue')]\n",
      "[('the hour of the concerns', ['include', 'includes'], 'includes'), ('the day of the presidents', ['have', 'has'], 'has')]\n",
      "[('the banks of the way', ['buy', 'buys'], 'buy'), ('the markets of the data', ['take', 'takes'], 'take')]\n"
     ]
    }
   ],
   "source": [
    "print(no_attractors_si[0:2])\n",
    "print(no_attractors_pl[0:2])\n",
    "print(one_attractor_si_same[0:2])\n",
    "print(one_attractor_pl_same[0:2])\n",
    "print(one_attractor_si_diff[0:2])\n",
    "print(one_attractor_pl_diff[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2b\n",
    "err_no_attractors_si = calculate_error_rate(no_attractors_si)\n",
    "err_no_attractors_pl = calculate_error_rate(no_attractors_pl)\n",
    "err_one_attractor_si_same =  calculate_error_rate(one_attractor_si_same)\n",
    "err_one_attractor_pl_same = calculate_error_rate(one_attractor_pl_same)\n",
    "err_one_attractor_si_diff = calculate_error_rate(one_attractor_si_diff)\n",
    "err_one_attractor_pl_diff = calculate_error_rate(one_attractor_pl_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproduce plot from the paper 2b\n",
    "N = 2\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.1       # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yvals = [err_no_attractors_pl, err_no_attractors_si]\n",
    "rects1 = ax.bar(ind, yvals, width, color='b')\n",
    "zvals = [err_one_attractor_pl_same, err_one_attractor_si_diff]\n",
    "rects2 = ax.bar(ind+width, zvals, width, color='g')\n",
    "kvals = [err_one_attractor_pl_diff, err_one_attractor_si_same ]\n",
    "rects3 = ax.bar(ind+width*2, kvals, width, color='r')\n",
    "\n",
    "ax.set_ylabel('Error rate')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('Plural subject', 'Singular subject') )\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0]), ('None', 'Plural', 'Singular') )\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.show()\n",
    "fig.savefig('2b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error plural subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_pl, err_one_attractor_pl_same,err_one_attractor_pl_diff))\n",
    "print(\"Erro singular subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_si,err_one_attractor_si_diff,err_one_attractor_si_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with 0-4 attractors \n",
    "def gen_num_attractors(num_sentences, num_words, num_attractors, NN, NNS, VBP, VBZ, helper):\n",
    "    assert(num_attractors >= 0)\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences = []\n",
    "    store_indices = []\n",
    "    for i in range(num_sentences):\n",
    "        x,y = np.random.randint(2, size=2)\n",
    "        while True:\n",
    "            indices = np.random.randint(num_words, size=2+num_attractors+1)\n",
    "            if tuple(indices) not in store_indices:\n",
    "                store_indices.append(tuple(indices))\n",
    "                break\n",
    "#  all combinations of homogenous interventions: 2c_mixed.pdf\n",
    "#\n",
    "        if(x == 0):\n",
    "            sent = f\"the {NN[indices[0]]}\"\n",
    "        else:\n",
    "            sent = f\"the {NNS[indices[0]]}\"\n",
    "        for j in range(1,num_attractors+1):\n",
    "            index = np.random.randint(len(helper), size=1)[0]\n",
    "            if(y == 0):\n",
    "                sent += f\" {helper[index]} {NNS[indices[j]]}\"\n",
    "            else:\n",
    "                sent += f\" {helper[index]} {NN[indices[j]]}\"\n",
    "        if(x == 0):\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "        else:\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBP[indices[-1]]))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "no_attr = gen_num_attractors(num_sentences, num_words, 0, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "one_attr = gen_num_attractors(num_sentences, num_words, 1, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "two_attr = gen_num_attractors(num_sentences, num_words, 2, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "three_attr = gen_num_attractors(num_sentences, num_words, 3, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "four_attr = gen_num_attractors(num_sentences, num_words, 4, NN, NNS, VBP, VBZ, attractor_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2c for singular and plural subjects\n",
    "err_no_attr = calculate_error_rate(no_attr)\n",
    "err_one_attr = calculate_error_rate(one_attr)\n",
    "err_two_attr = calculate_error_rate(two_attr)\n",
    "err_three_attr = calculate_error_rate(three_attr)\n",
    "err_four_attr = calculate_error_rate(four_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for number of attractors: (0, 1, 2, 3, 4) \n",
      "(0.18200000000000005, 0.401, 0.45299999999999996, 0.488, 0.496)\n"
     ]
    }
   ],
   "source": [
    "print(\"Error for number of attractors: (0, 1, 2, 3, 4) \")\n",
    "print((err_no_attr, err_one_attr, err_two_attr, err_three_attr, err_four_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt0FfW5//H3wzVc5CZXjQhBKhYh\nghHxghUh1oBoigIKClrUamurVesVtSo/rQtrWz2lFo4esHIVoaCAEhHBy1EJVUDliAVRFAREqFxE\nQvL8/tgTDJhkdiA7k2R/XmuxMjN7Lk9m6f7kO9+Z75i7IyIiUpoaURcgIiKVn8JCRERCKSxERCSU\nwkJEREIpLEREJJTCQkREQiksREQklMJCRERCKSxERCRUragLKC/Nmzf3du3aRV2GiEiVsmzZsq/c\nvUXYetUmLNq1a0dubm7UZYiIVClm9mk86+kylIiIhFJYiIhIKIWFiIiEUliIiEgohYWIiIRSWIiI\nSCiFhYiIhFJYiIhIKIWFiIiEUliIiEgohYWIiIRSWIiISCiFhYiIhFJYiIhIqISGhZmtM7OVZvae\nmeUGy5qZWY6ZfRz8bBosv8jMPjCz18zsyGBZBzObmsgaRUQkXEW0LHq7+0nunhHM3w4sdPeOwMJg\nHuBmoCfwNDA0WDYauLsCahQRkVJEcRnqQmBiMD0RyA6mC4C6QH0gz8x6ARvd/eOKL1FERIpK9Jvy\nHFhgZg783d3HAa3cfSOAu280s5bBuvcBLwEbgMuA6cAlCa5PRETikOiwOMPdNwSBkGNm/1fSiu6e\nA+QAmNkIYB5wvJndAmwDbnD33UW3MbNrgGsA2rZtm6BfQUSkcnJ3vsv/DoCUWikJPVZCw8LdNwQ/\nN5vZLKAHsMnM2gStijbA5qLbmFl9YATwU2ABsctWQ4FhwPiD9j8OGAeQkZHhifxdRETKyt3Zs28P\nu/J2sTtvN7v27jpgenfebnbl7Sp5OmTd3Xm7KfAC7jjzDh7s82BCf5eEhYWZNQBquPuOYPpc4H5g\nDrEw+EPwc/ZBm94K/MXd88ysHrFLWQXE+jJERMqNu/Ptvm+L/SIP/YKPc12nbH/H1q1ZlwZ1GlC/\ndn0a1G6wf7pJShOObnT098trB+vUacDpx5yeoDP0vUS2LFoBs8ys8DiT3f1FM1sKTDezkcBnwKDC\nDczsKCDD3X8fLPoj8Bawne87wkUkSRR4Ad/mfVviX9VxfcGHfF5WKbVSDviiLvzyblavGamNUn/w\nRV7cuiVN169dn5o1aibgTB4+c68eV28yMjI8Nzc36jJEpAwKvIB129exYtMKln+5nBWbV/D+5vf5\n+tuv2Z23+5C+zOvVqhf+5Vyr7F/kDeo0oF6tepX2y/xQmdmyIo82lCjRHdwiIgDs+G4H729+n+Wb\nlsfCYdNyVm5ayY69OwAwjI5HdqRLyy60bNCyzF/k9WvXp37t+tQwDUyRCAoLESlXxbUWln+5nDXb\n1uxfp3HdxnRt1ZUR6SPo2qor6a3T6dyiMw3qNIiwcimNwkJEDtnOvTtZuWllaGuhW5tuXHHSFaS3\nSqdrq660bdyWoD9TqgiFhYiEUmtBFBYicoDC1kJhSyGstdC1VVfSW6WrtVDNKSxEklRZWgvD04fv\nv4R0YssT1VpIQgoLkSQQT2vhuGbHqbUgJVJYiFQjBV7Ap9s/jQVCCa2FRnUbkd4qXa0FKROFhUgV\ndXBrYcWmFazYtKLY1sKI9BGkt05Xa0EOmcJCpJJzd9ZtX6fWgkRKYSFSiRxKa6Frq64c2/hYtRYk\noRQWIhEo2looGgxrvl6zf5TSRnUb6U4kqTQUFiIJtnPvztiYSF8uL7W1cFLrkxjedbhaC1IpKSxE\nyllefh5vf/E2OWtyWLB2Ae988Q4FXgAc2FoovD1VrQWpChQWIofJ3flo60fkrMkhZ20Or657lR17\nd1DDanDKUadwx5l3cMpRp5DeOl2tBamyFBYih2DLri0s/GTh/oBY/816ANKapjG0y1Ay0zI5p/05\nNK3XNOJKRcqHwkIkDnv27eGNz94gZ20OC9Ys4N0v3wWgSUoT+rTvw1297iKzQyZpTdMirlQkMRQW\nIsVwd1ZsWkHO2ljLYcmnS9izbw+1atTi9GNO54HeD5CZlknGURnV7s1pIsVRWIgENuzYsP+y0str\nX2bTrk0AnND8BH5x8i/ITMvkJ+1+QsM6DSOuVKTiKSwkae3au4vFny5mwZoF5KzN4cMtHwLQskFL\n+qb1JTMtk75pfUltlBpxpSLRU1hI0sgvyGfZxmX7Ww9vrn+TvII8Umql0KttL65Iv4JzO5xLl1Zd\n9B5nkYMoLKRa+2TbJ/v7HRauXci2PdsA6Na6G7/t+VsyO2RyZtszSamVEnGlIpWbwkKqle17tvPK\nJ6/sbz0UDraX2iiV7E7ZZKZl0ietDy0btIy4UpGqRWEhVVpefh5vff7W/tZD4dPSDes0pHe73txw\n6g1kdsjk+COP18NwIodBYSFVStGnpResXcCr615l596d1LAa9Di6R+x5h7RMeqb2pHbN2lGXK1Jt\nKCyk0tuyawsvr315f+vh828+B6BD0w5c1uUyzu1wLr3b96ZJSpOIKxWpvhQWUuns2beH1z97fX+/\nQ+HT0k1TmtInrQ+ZaZlkpmXSvmn7iCsVSR4KC4lcgRewctPK/c87vPbZa+zZt4faNWpz+jGnM7r3\naM7tcC7d23TX09IiEVFYSCS++OaL/ZeVXl77Mpt3bQagc4vOXHvytWR2yOSsY8/S09IilYTCQirE\nzr07Wbxu8f6B+FZ9tQqAVg1a7b+s1DetL0c3OjriSkWkOAoLSYj8gnxyN+Tubz387/r/3f+09FnH\nnsXIbiPJ7JBJl5ZddEurSBWQ8LAws5pALvCFu59vZu2BqUAz4F/A5e6+18x+DfwC+AzIDpadCQx0\n95sSXaccvrXb1u7vlF74yUK279kOQPc23bnptJvITMvkjLZn6GlpkSqoIloWNwCrgEbB/MPAn9x9\nqpk9AYwE/gZcBXQFHgB+amYvAHcDl1RAjXIItn27Lfa0dNB6WLttLQDHNDqGgZ0Gcm6Hc+mT1ofm\n9ZtHXKmIHK6EhoWZpQL9gf8H3GSx6w3nAEODVSYCvycWFgC1gfpAHnA5MM/dtyWyRimb3Xm7eeTN\nR5j38TyWblhKgRdwRJ0j6N2+d2yspbRMfnTkj3RpSaSaSXTL4s/ArcARwfyRwHZ33xfMfw4U9mg+\nArwFfAC8AfwTOC/B9UkZ5BfkM/S5ocz5aA49U3syqtcoMjtkcurRp+ppaZFqLmFhYWbnA5vdfZmZ\nnV24uJhVHcDd/wH8I9j2XuAxIMvMhgPrgZvdveCgY1wDXAPQtm3bRPwaEnB3bnjxBmZ/NJvHsx7n\n+h7XR12SiFSgRA7afwZwgZmtI9ahfQ6xlkYTMysMqVRgQ9GNzOwo4BR3nw2MAoYA3wF9Dj6Au49z\n9wx3z2jRokXCfhGBR958hL8u/Su3nHaLgkIkCSUsLNz9DndPdfd2xDqpX3H3YcAi4OJgtRHA7IM2\nfYBYxzZAPWItjwJifRkSganvT+XWl29lSOchPJz5cNTliEgEongd2G3EOrv/TawP48nCD8ysG4C7\nvxssehJYCXQHXqzgOgVYvG4xI/45grOOPYsJ2RP0BjmRJGXuHnUN5SIjI8Nzc3OjLqNa+XDLh5zx\n1Bm0btiaN37+Bs3qNYu6JBEpZ2a2zN0zwtbTn4lSrI07NpI1KYuUWinMHzZfQSGS5DTch/zAju92\n0H9yf7bu3sqSK5fQrkm7qEsSkYgpLOQAefl5DJ4xmBWbVvD8pc/TvU33qEsSkUpAYSH7uTvXzb2O\nF//9IuMHjCerY1bUJYlIJaE+C9lv9JLRPPnuk4zqNYqrul8VdTkiUokoLASAie9N5J5X72F4+nDu\n731/1OWISCWjsBBy1uRw1fNX0TetL+MHjNcggCLyAwqLJLf8y+VcNP0iTmh+AjMGzaBOzTpRlyQi\nlZDCIomt/896+k3uR6O6jZg3bB6NUxpHXZKIVFK6GypJbd+znX6T+7Fz705ev/J1UhulRl2SiFRi\nCosktDd/LwOnDeSjrz5i/rD5dGnVJeqSRKSSU1gkGXdn5JyRLFq3iKezn6ZP2g9GfhcR+QH1WSSZ\nUa+M4pkVzzC692guT7886nJEpIpQWCSRccvG8eDrD3J196u5s9edUZcjIlWIwiJJzF09l+vmXke/\njv0Y23+snqUQkTJRWCSB3A25DJ4xmG6tuzHt4mnUqqGuKhEpG4VFNffJtk/oP7k/LRu05IWhL9Cw\nTsOoSxKRKkhhUY1t3b2VrElZ5OXnMX/YfFo3bB11SSJSRel6RDW1Z98eLpx6Ieu2ryPn8hw6Ne8U\ndUkiUoUpLKqhAi9g+KzhvLH+DaZdPI1ex/aKuiQRqeJ0Gaoa+t2C3/Hsh8/ySOYjDO48OOpyRKQa\niCsszOxMM7symG5hZu0TW5YcqsfefoxH33qUX/f4NTeddlPU5YhINREaFmZ2L3AbcEewqDbwTCKL\nkkMzc9VMbnzxRrI7ZfOnn/5Jz1KISLmJp2XxM+ACYBeAu28AjkhkUVJ2b65/k2Ezh3Fq6qlMGjiJ\nmjVqRl2SiFQj8YTFXnd3wAHMrEFiS5KyWr11NRdMuYDURqnMuWQO9WvXj7okEalm4gmL6Wb2d6CJ\nmV0NvAz8d2LLknht3rWZrElZmBnzh82nRYMWUZckItVQ6K2z7v6ImWUC3wDHA/e4e07CK5NQu/bu\nYsCUAWzcsZFXRrzCcc2Oi7okEammQsPCzB5299uAnGKWSUTyC/IZOnMoS79YyswhM+mZ2jPqkkSk\nGovnMlRmMcuyyrsQiZ+785v5v2HOR3N4LOsxsjtlR12SiFRzJbYszOw64JdAmpmtKPLREcAbiS5M\nSjbmzTGMzR3LLafdwvU9ro+6HBFJAqVdhpoMzAceAm4vsnyHu3+d0KqkRFNWTuG2l29jSOchPJz5\ncNTliEiSKPEylLv/x93Xuful7v4p8C2x22cbmlnbsB2bWYqZvWNmy83sAzO7L1je3szeNrOPzWya\nmdUJlv/azN43s3lFlp1pZo+Wy29aDSxet5grZl/BWceexYTsCdQwjdYiIhUjnie4B5jZx8AnwGJg\nHbEWR5jvgHPcPR04CTjPzHoCDwN/cveOwDZgZLD+VUBX4F3gpxZ7/Phu4IEy/UbV1IdbPiR7WjZp\nTdOYNWQWKbVSoi5JRJJIPH+ajgZ6AqvdvT3Qhzj6LDxmZzBbO/jnwDnAjGD5RKBo72xtoD6QB1wO\nzHP3bXHUWK1t2LGBrElZpNRKYf6w+TSr1yzqkkQkycQTFnnuvhWoYWY13H0RsZZCKDOraWbvAZuJ\n3Xq7Btju7vuCVT4Hjg6mHwHeAloQC6MRwNiQ/V9jZrlmlrtly5Z4Sqpydny3g/6T+7N191bmDp1L\nuybtoi5JRJJQPGGx3cwaAkuASWb2F2BfyDYAuHu+u58EpAI9gBOKWy1Y9x/u3s3dLwNuAh4Dssxs\nhpn9yeyHF+jdfZy7Z7h7RosW1e/J5bz8PAY9O4iVm1by7KBn6d6me9QliUiSiicsLgR2A78FXiTW\nOhhQloO4+3bgVWKXs5qYWeFdWKnAhqLrmtlRwCnuPhsYBQwh1v/RpyzHrOrcnWtfuJaX1rzEE+c/\nQVZHPdoiItEpNSzMrCYw290L3H2fu09098eCy1KlCt570SSYrgf0BVYBi4CLg9VGALMP2vQBYh3b\nAPWItTwKiPVlJI0HljzAU+89xaheo7iq+1VRlyMiSa7UsHD3fGC3mTU+hH23ARYFD/QtBXLc/QVi\n78a4ycz+DRwJPFm4gZl1C477brDoSWAl0J1YqyYpTHhvAve+ei/D04dzf+/7oy5HRASLjT5eygpm\n04ldPsoheKcFgLv/JrGllU1GRobn5uZGXcZhW7BmAf0n9+fsdmczd+hc6tSsE3VJIlKNmdkyd88I\nWy90IEFgbvBPEmz5l8u5ePrFnND8BGYMmqGgEJFKI54hyidWRCHJbv1/1tNvcj8a1W3EvGHzaJxy\nKFf+REQSI56WhSTY9j3byZqUxc69O3n9ytdJbZQadUkiIgdQWERsb/5eBk4byOqtq5k/bD5dWnWJ\nuiQRkR8IvXXWzMZUVDHJxt0ZOWcki9Yt4skLnqRPWlI9SiIiVUg8t86eHAzqJ+Vs1CujeGbFM4zu\nPZrL0y+PuhwRkRLFcxnqXWC2mT3LgbfOzkxYVUlg3LJxPPj6g1zd/Wru7HVn1OWIiJQqnrBoBmwl\nNlpsIQcUFodo7uq5XDf3Ovp17MfY/mNRw01EKrt4bp29siIKSRa5G3IZPGMw3Vp3Y9rF06hVQ/cY\niEjlF8/Lj1LNbJaZbTazTWb2nJnp3s5D8Mm2T+g/uT8tG7TkhaEv0LBOw6hLEhGJSzyjzv4PMAc4\niti7J54PlkkZbN29laxJWeTl5zF/2HxaN2wddUkiInGLJyxauPv/BKPO7nP3CcReUCRx2rNvDxdO\nvZB129cx+5LZdGreKeqSRETKJJ6w+MrMLgueuahpZpcR6/CWOBR4AcNnDeeN9W/w9M+eptexvaIu\nSUSkzOIJi58Dg4EvgY3E3kXx80QWVZ38bsHvePbDZxmTOYbBnQdHXY6IyCEp9Vac4OVHF7n7BRVU\nT7Xy2NuP8ehbj3L9Kddz82k3R12OiMghi+cJ7gsrqJZqZeaqmdz44o1kd8rmz+f9Wc9SiEiVFs9N\n/m+Y2X8B0zjwCe5/JayqKu7N9W8ybOYwTk09lUkDJ1GzRs2oSxIROSzxhMXpwc+i7/d0DnyiWwKr\nt67mgikXkNoolTmXzKF+7aR6dbiIVFNhfRY1gL+5+/QKqqdK27xrM1mTsjAz5g+bT4sGusNYRKqH\nsD6LAuD6CqqlStu1dxcDpgxg446NPH/p8xzX7LioSxIRKTfx3DqbY2a3mNkxZtas8F/CK6tC8gvy\nGTpzKEu/WMrkiybTM7Vn1CWJiJSrePosCp+p+FWRZQ6klX85VY+785v5v2HOR3N4POtxsjtlR12S\niEi5i2fU2fYVUUhVNebNMYzNHcstp93C9T10xU5EqqcSL0OZ2a1Fpgcd9NmDiSyqqpiycgq3vXwb\nQzoP4eHMh6MuR0QkYUrrs7ikyPQdB312XgJqqVIWr1vMFbOv4Kxjz2JC9gRqWDzdPyIiVVNp33BW\nwnRx80nlwy0fkj0tm7SmacwaMouUWilRlyQiklClhYWXMF3cfNLYsGMDWZOySKmVwvxh82lWTzeG\niUj1V1oHd7qZfUOsFVEvmCaYT8o/pXd8t4P+k/uzdfdWlly5hHZN2kVdkohIhSgxLNxdAxoVkZef\nx6BnB7Fy00qev/R5urfpHnVJIiIVJp7nLJKeu3PtC9fy0pqXGD9gPFkds6IuSUSkQiXsFp7gie9F\nZrbKzD4wsxuC5c3MLMfMPg5+Ng2WXxSs95qZHRks62BmUxNVY7weWPIAT733FKN6jeKq7ldFXY6I\nSIVL5P2e+4Cb3f0EoCfwKzP7MXA7sNDdOwILg3mAm4P1ngaGBstGA3cnsMZQE96bwL2v3svw9OHc\n3/v+8A1ERKqhhIWFu28sfOeFu+8AVgFHE3uZ0sRgtYlA4fgYBUBdoD6QZ2a9gI3u/nGiagyzYM0C\nrn7+avqm9WX8gPF6gZGIJK0K6bMws3ZAN+BtoJW7b4RYoJhZy2C1+4CXgA3AZcB0DnwwsEIt/3I5\nF0+/mBOan8CMQTOoU7NOVKWIiEQu4Y8dm1lD4DngRnf/pqT13D3H3U929wHEWhvzgOPNbIaZjTez\nH7xFyMyuMbNcM8vdsmVLudW8/j/r6Te5H43qNmLesHk0TmlcbvsWEamKEhoWZlabWFBMcveZweJN\nZtYm+LwNsPmgbeoDI4CxwEPERr1dBgw7eP/uPs7dM9w9o0WL8nnR0PY928malMXOvTuZP2w+qY1S\ny2W/IiJVWSLvhjLgSWCVuz9a5KM5xMKA4Ofsgza9FfiLu+cB9Yg9LV5ArC8job7b9x0Dpw1k9dbV\nzBw8ky6tuiT6kCIiVUIi+yzOAC4HVprZe8GyO4E/ANPNbCTwGbB/RFszOwrIcPffB4v+CLwFbOf7\njvCEcHdGzhnJonWLeDr7afqk9Unk4UREqpSEhYW7v07JAw4W+03s7huA84vMPws8W/7V/dBdr9zF\npJWTGN17NJenX14RhxQRqTI0rjbw99y/89DrD3F196u5s9edUZcjIlLpJH1YzPt4Hr+c90v6dezH\n2P5j9SyFiEgxkj4sOjXvxODOg5l28TRq1dBQWSIixUn6b8e0pmlMuWhK1GWIiFRqSd+yEBGRcAoL\nEREJpbAQEZFQCgsREQmlsBARkVAKCxERCaWwEBGRUAoLEREJpbAQEZFQCgsREQmlsBARkVAKCxER\nCaWwEBGRUAoLEREJpbAQEZFQCgsREQmlsBARkVAKCxERCaWwEBGRUAoLEREJpbAQEZFQCgsREQml\nsBARkVAKCxERCaWwEBGRUAoLEREJpbAQEZFQCQsLM3vKzDab2ftFljUzsxwz+zj42TRYfpGZfWBm\nr5nZkcGyDmY2NVH1iYhI/BLZspgAnHfQstuBhe7eEVgYzAPcDPQEngaGBstGA3cnsD4REYlTwsLC\n3ZcAXx+0+EJgYjA9EcgOpguAukB9IM/MegEb3f3jRNUnIiLxq1XBx2vl7hsB3H2jmbUMlt8HvARs\nAC4DpgOXhO3MzK4BrgFo27ZtQgoWEZFK0sHt7jnufrK7DyDW2pgHHG9mM8xsvJnVL2G7ce6e4e4Z\nLVq0qNCaRUSSSUWHxSYzawMQ/Nxc9MMgFEYAY4GHgJ8Dy4BhFVyniIgUUdFhMYdYGBD8nH3Q57cC\nf3H3PKAe4MT6M4ptWYiISMVIWJ+FmU0Bzgaam9nnwL3AH4DpZjYS+AwYVGT9o4AMd/99sOiPwFvA\ndr7vCBcRkQiYu0ddQ7nIyMjw3NzcqMsQEalSzGyZu2eErVcpOrhFRKRyU1iIiEgohYWIiIRSWIiI\nSCiFhYiIhFJYiIhIKIWFiIiEUliIiEgohYWIiIRSWIiISCiFhYiIhFJYiIhIKIWFiIiEUliIiEgo\nhYWIiIRSWIiISCiFhYiIhFJYiIhIKIWFiIiEUliIiEgohYWIiIRSWIiISCiFhYiIhFJYiIhIKIWF\niIiEUliIiEgohYWIiIRSWIiISCiFhYiIhFJYiIhIqEjCwszOM7OPzOzfZnZ7sGySma0wsweLrHe3\nmV0YRY0iIvK9Cg8LM6sJ/BXIAn4MXGpmXQHcvSvQy8wam1kboIe7z67oGkVE5EC1IjhmD+Df7r4W\nwMymAv2BemZWA6gD5AP3A/dEUJ+IiBwkistQRwPri8x/Hiz7DPgXMB04DjB3f7fiyxMRkYNF0bKw\nYpa5u9+4fwWz54FfmNldQDqQ4+7jf7Ajs2uAa4LZnWb20SHW1Bz46hC3TVY6Z2Wj81U2Ol9lczjn\n69h4VooiLD4HjikynwpsKJwJOrRzgQbAie4+2MyWmNkkd99ddEfuPg4Yd7gFmVmuu2cc7n6Sic5Z\n2eh8lY3OV9lUxPmK4jLUUqCjmbU3szrAJcAcADOrDdwAjAHqA16kzjoR1CoiIkTQsnD3fWZ2PfAS\nUBN4yt0/CD7+FTDR3Xeb2QrAzGwlMM/dt1d0rSIiEhPFZSjcfR4wr5jlfy4y7cClFVTSYV/KSkI6\nZ2Wj81U2Ol9lk/DzZbHvZBERkZJpuA8REQmV9GFR3NAjUjwze8rMNpvZ+1HXUhWY2TFmtsjMVpnZ\nB2Z2Q9Q1VWZmlmJm75jZ8uB83Rd1TVWBmdU0s3fN7IVEHiepw6KEoUd+HG1VldoE4Lyoi6hC9gE3\nu/sJQE/gV/rvq1TfAee4ezpwEnCemfWMuKaq4AZgVaIPktRhQZGhR9x9LzAV0MCFJXD3JcDXUddR\nVbj7Rnf/VzC9g9j/0EdHW1Xl5TE7g9nawT91qpbCzFKJDZf034k+VrKHRUlDj4iUKzNrB3QD3o62\nksotuKTyHrCZ2MgNOl+l+zNwK1CQ6AMle1gUO/RIhVch1ZqZNQSeA25092+irqcyc/d8dz+J2MgO\nPczsxKhrqqzM7Hxgs7svq4jjJXtYlDr0iMjhCkYleA6Y5O4zo66nqggewn0V9ZGV5gzgAjNbR+wS\n+jlm9kyiDpbsYVHi0CMih8vMDHgSWOXuj0ZdT2VnZi3MrEkwXQ/oC/xftFVVXu5+h7ununs7Yt9d\nr7j7ZYk6XlKHhbvvAwqHHlkFTC8y9IgcxMymAP8LHG9mn5vZyKhrquTOAC4n9hffe8G/flEXVYm1\nARYFQ/0sJdZnkdDbQSV+eoJbRERCJXXLQkRE4qOwEBGRUAoLEREJpbAQEZFQCgsREQmlsJAqw8xa\nm9lUM1tjZh+a2Twz+1E5H+NsMzu9jNvUNbOXg1tjh8S5TXbRQQXN7AozO6qs9Zaw7yZm9svy2JdI\nIYWFVAnBA26zgFfdvYO7/xi4E2hVzoc6GyhTWBAb86m2u5/k7tPi3Cab2EjHha4Aig2LYHTksmgC\nlCksDuEYkmQUFlJV9Aby3P2JwgXu/p67v2YxY8zsfTNbWfjXfdBK2P9Ql5n9l5ldEUyvM7P7zOxf\nwTadgsH+rgV+G7QSehUtwMyamdk/zWyFmb1lZl3NrCXwDHBSsE2Hg7a52syWBu9oeM7M6gctlwuA\nMcE2twEZwKRgvl5Q3z1m9jowqLj9BPtvZWazguXLg33/AegQ7GtMyPlZZGaTgZVm1sDM5gb7eT/e\nVpIkh0jewS1yCE4EShowbSCx9x+kA82BpWa2JI59fuXu3YNLNre4+1Vm9gSw090fKWb9+4B33T3b\nzM4Bnnb3k8zsqmD784vZZqa7jwcws9HASHd/3MzmAC+4+4zgs6xgH7nBPMAedz8zmD/y4P0AjwOP\nAYvd/WdB66AhcDtwYjAgH2Z2USnnp0ew7ifBehvcvX+wXeM4zqEkCbUspDo4E5gSjFi6CVgMnBLH\ndoUD+y0D2sV5nH8AuPsrwJFxfKGeaGavmdlKYBjQOY7jFCp6Sauk/ZwD/C2oKd/d/1NC3SWdn3fc\n/ZNgeiXQ18weNrNeJexLkpRmPcObAAABcklEQVTCQqqKD4CTS/isuKHmIfamuqL/jacc9Pl3wc98\n4mtlH8qQ9hOA6929C7GWycE1lGZXOe2npPNzwDHcfTWxc7wSeMjM7inDMaSaU1hIVfEKUNfMri5c\nYGanmNlPgCXAEIu9OKcFcBbwDvAp8OPgbqXGQJ84jrMDOKKEz5YQ+6seMzub2GWssPdTHAFstNhQ\n5cNKOU5pxy1tPwuB64KaappZo2L2VdL5OUBwN9Zud38GeAToHvK7SRJRWEiV4LERL38GZFrs1tkP\ngN8Te//ILGAFsJxYqNzq7l+6+3pgevDZJODdOA71PPCz4jq4g+NlWGxU1D8AI+LY393E3o6Xw4HD\nbU8Ffmdm7wad4hOAJwo7uMuwnxuA3sHlqWVAZ3ffCrwRdFKPoYTzU8wxugDvWOxNdXcBo+P4/SRJ\naNRZEREJpZaFiIiEUliIiEgohYWIiIRSWIiISCiFhYiIhFJYiIhIKIWFiIiEUliIiEio/w+D8KOE\nm8AidAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b4c0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot error for 2c for singular and plural subjects: 2c_not_mixed.pdf\n",
    "err = [err_no_attr, err_one_attr,err_two_attr,err_three_attr,err_four_attr]\n",
    "\n",
    "x = np.arange(0,5) \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel('Error rate')\n",
    "\n",
    "ax1.set_xlabel('Count of attractors')\n",
    "ax1.plot(x, err, color='g')\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax1.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "ax1.set_ylim([0,0.55])\n",
    "ax1.legend(loc=2)\n",
    "plt.show()\n",
    "fig.savefig('2c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the taxes', ['make', 'makes'], 'make'), ('the president', ['argue', 'argues'], 'argues'), ('the investment', ['come', 'comes'], 'comes'), ('the president', ['offer', 'offers'], 'offers')]\n",
      "\n",
      "[('the hours close to the investments', ['argue', 'argues'], 'argue'), ('the unit close to the executive', ['offer', 'offers'], 'offers'), ('the quarter by the years', ['take', 'takes'], 'takes'), ('the concerns close to the ways', ['provide', 'provides'], 'provide')]\n",
      "\n",
      "[('the company in the lines close to the plans', ['need', 'needs'], 'needs'), ('the weeks of the month close to the line', ['remain', 'remains'], 'remain'), ('the investments in the executives close to the sales', ['fear', 'fears'], 'fear'), ('the investment in the sales of the bids', ['hope', 'hopes'], 'hopes')]\n",
      "\n",
      "[('the line by the company of the company in the tax', ['take', 'takes'], 'takes'), ('the government close to the concern in the investment close to the plan', ['think', 'thinks'], 'thinks'), ('the days by the taxes of the states in the parts', ['need', 'needs'], 'need'), ('the loss of the shares in the hours by the hours', ['buy', 'buys'], 'buys')]\n",
      "\n",
      "[('the interests in the shares of the businesses by the plans of the executives', ['suggest', 'suggests'], 'suggest'), ('the programs in the executive in the president in the system close to the group', ['see', 'sees'], 'see'), ('the weeks of the plant by the part in the business in the group', ['fear', 'fears'], 'fear'), ('the tax of the executive close to the data by the executive close to the market', ['pay', 'pays'], 'pays')]\n"
     ]
    }
   ],
   "source": [
    "# print sentences that incorporate 0-4 homogenous interventions\n",
    "print(no_attr[0:4])\n",
    "print()\n",
    "print(one_attr[0:4])\n",
    "print()\n",
    "print(two_attr[0:4])\n",
    "print()\n",
    "print(three_attr[0:4])\n",
    "print()\n",
    "print(four_attr[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
