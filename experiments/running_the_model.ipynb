{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Grammar in Neural language models\n",
    "\n",
    "We will investigate a language model trained on the penn treebank dataset, using the code provided at https://github.com/pytorch/examples/tree/master/word_language_model. The model consists of an encoder with 2 hidden LSTM layers with 1500 units, and a linear output layer to which a softmax function is applied. The word embeddings have dimensionality 1500. The model is trained for 40 epochs with a dropout factor of 0.65 and has a test perplexity of 72.30 on the test set. If you are interested in more detail in the model, we advise you to look at the repository containing the code.\n",
    "\n",
    "In this notebook, we will walk you through an example of how you can compute the probabilties of the next word in a sentence. You can then use this to start your replication of Linzen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2834)\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib as matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you downloaded and extracted the zipfile, you should have all data required: the model, and the pickled dictionary mapping words to indices.\n",
    "\n",
    "Lets start by loading the model. Because the model was trained on a GPU, we need to specifically say that it should be loaded on the CPU when we load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel (\n",
      "  (drop): Dropout (p = 0.65)\n",
      "  (encoder): Embedding(10000, 1500)\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
      "  (decoder): Linear (1500 -> 10000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the architecture of your model\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a single sentence\n",
    "\n",
    "We will give an example of how you can get the probabilties for the next word in a single sentence. We will uset he example sentence:<br>\n",
    "\n",
    "\"This is a sentence with seven\"\n",
    "\n",
    "And print the probabilities of completing this sentence with either 'words', 'characters', 'thursday', 'days' or 'walk'. As the model itself does not include the mapping from words to indices, we will need to do this as a preprocessing step. The dictionary that maps words to indices is stored in a pickled file called 'dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that can be used to evaluate a single sentence and print the probabilities of finishing this sentence with a word from a list of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    #print (sentence, '\\n')\n",
    "    #print ('\\n'.join(\n",
    "    #        ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "    #        ) )\n",
    "\n",
    "    return {word : get_prob(word) for word in check_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test sentence and words to check\n",
    "test_sentence = 'this is a sentence with seven'\n",
    "check_words = ['words', 'characters', 'thursday', 'days', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': 7.000279583735391e-05,\n",
       " 'days': 0.002636460354551673,\n",
       " 'thursday': 1.165580556516943e-06,\n",
       " 'walk': 5.386583779909415e-06,\n",
       " 'words': 0.0009886504849418998}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it yourself\n",
    "\n",
    "You can now start with your replication of Linzen's paper, for which the first step is to try different inputs with varying distances etc. to get a feeling for what the model is doing and to familiarise yourself with using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are': 0.03104715794324875, 'is': 0.0033276162575930357}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"the toy on the tables\"\n",
    "check_words = [\"is\", \"are\"]\n",
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentence, options, correct-option):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the product of the company', ['looks', 'look'], 'looks'),\n",
       " ('the products of the company', ['looks', 'look'], 'look'),\n",
       " ('the product of the companies', ['looks', 'look'], 'looks'),\n",
       " ('the products of the companies', ['looks', 'look'], 'look'),\n",
       " ('the product that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the products that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the product that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the products that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the product that the company produces', ['looks', 'look'], 'looks'),\n",
       " ('the products that the company produces', ['looks', 'look'], 'look'),\n",
       " ('the product that the companies produce', ['looks', 'look'], 'looks'),\n",
       " ('the products that the companies produce', ['looks', 'look'], 'look')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose sentence prefixes with frequent words.\n",
    "# The sentence prefixes are intended to test intervening nouns.\n",
    "\n",
    "\n",
    "NN = ['company', 'year', 'market', 'share', 'stock', 'system', 'president', 'business', \n",
    "      'quarter', 'government', 'time', 'week', 'price', 'group', 'interest',\n",
    "      'industry', 'unit','month', 'rate', 'investment', 'state', 'producer', 'income', \n",
    "      'program', 'bank', 'part', 'plan', 'sale', 'issue', 'tax', 'way', 'loss', 'executive', 'day', 'bid', 'data', 'line','hour', 'plant', 'concern']\n",
    "\n",
    "NNS = ['companies', 'years', 'markets', 'shares', 'stocks', 'systems', 'presidents', \n",
    "       'businesses', 'quarters', 'governments', 'times', 'weeks', 'prices', 'groups', 'interests', 'industries', \n",
    "       'units', 'months', 'rates', 'investments', 'states', 'producers', 'incomes', 'programs', 'banks', 'parts', 'plans', \n",
    "      'sales', 'issues', 'taxes', 'ways', 'losses', 'executives', 'days', 'bids', 'data', 'lines', 'hours', 'plants', 'concerns',]\n",
    "\n",
    "VBP = ['are', 'have', 'do', 'say', 'think', 'want', 'expect', 'include', 'ask', \n",
    "       'make', 'need', 'know', 'see', 'get', 'seem', 'remain', 'continue', 'show', 'buy', \n",
    "       'feel', 'go', 'sell', 'take', 'use', 'plan', 'look', 'tend', 'hope', 'argue', 'give',\n",
    "       'pay', 'appear', 'suggest', 'fear', 'find', 'come', 'offer', 'contend', 'agree', 'provide']\n",
    "\n",
    "VBZ = ['is', 'has', 'does', 'says', 'thinks', 'wants', 'expects', 'includes', 'asks', 'makes',\n",
    "      'needs', 'knows', 'sees', 'gets', 'seems', 'remains', 'continues', 'shows', 'buys', 'feels', 'goes', 'sells',\n",
    "      'takes', 'uses', 'plans', 'looks', 'tends', 'hopes', 'argues', 'gives', 'pays', 'appears', 'suggests', 'fears',\n",
    "      'finds', 'comes', 'offers', 'contends', 'agrees', 'provides']\n",
    "\n",
    "attractor_helpers = ['in the', 'by the', 'close to the', 'of the', 'at the', 'ant not the', 'without']\n",
    "\n",
    "words = {\n",
    "    \"NN1\" : \"product\",\n",
    "    \"NNS1\" : \"products\",\n",
    "    \"NN2\" : \"company\",\n",
    "    \"NNS2\" : \"companies\",\n",
    "    \"VBP1\" : \"looks\",\n",
    "    \"VBZ1\" : \"look\",\n",
    "    \"VBP2\" : \"produces\",\n",
    "    \"VBZ2\" : \"produce\",\n",
    "}\n",
    "sentences = [\n",
    "    (f\"the {words['NN1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1'])\n",
    "]\n",
    "\n",
    "print(\"(sentence, options, correct-option):\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_rate(sentences):\n",
    "    result = calculate_errors(sentences)\n",
    "    #print(result)\n",
    "    return 1- sum(result)/len(result)\n",
    "    \n",
    "def calculate_errors(sentences):\n",
    "    return [1 if is_correct_prediction(s[0], s[1], s[2]) else 0 for s in sentences]\n",
    "\n",
    "def is_correct_prediction(sentence, check_words, correct_word):\n",
    "    predictions = evaluate(lm, dictionary, sentence, check_words)\n",
    "    predicted_word = max(predictions, key=predictions.get)\n",
    "    return predicted_word == correct_word\n",
    "    \n",
    "calculate_error_rate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with one and without attractors\n",
    "\n",
    "def gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices_x = []\n",
    "    indices_u = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y = np.random.randint(num_words, size=2)\n",
    "            if (x,y) not in indices_x:\n",
    "                indices_x.append((x,y))\n",
    "                break\n",
    "        while True:\n",
    "            u,v = np.random.randint(num_words, size=2)\n",
    "            if (u,v) not in indices_u:\n",
    "                indices_u.append((u,v))\n",
    "                break\n",
    "        sentences_si.append((f\"the {NN[x]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "        sentences_pl.append((f\"the {NNS[u]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "def gen_one_attractor(num_sentences, num_words, same, NN, NNS, VBP, VBZ, \n",
    "                      template = \"the {} of the {}\", first_dep = True):\n",
    "\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y,z = np.random.randint(num_words, size=3)\n",
    "            if (x,y,z) not in indices:\n",
    "                indices.append((x,y,z))\n",
    "                break\n",
    "        if(same):\n",
    "            sentences_si.append((template.format(NN[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        elif first_dep:\n",
    "            sentences_si.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        else:\n",
    "            sentences_si.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "num_sentences = 1000\n",
    "num_words = len(NN)\n",
    "\n",
    "no_attractors_si, no_attractors_pl = gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_same, one_attractor_pl_same = gen_one_attractor(num_sentences, num_words,True, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_diff, one_attractor_pl_diff = gen_one_attractor(num_sentences, num_words,False, NN, NNS, VBP, VBZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_possesive, one_attractor_pl_diff_possesive = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} of the {}\", first_dep = True)\n",
    "\n",
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_relativizer, one_attractor_pl_diff_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} that the {}\", first_dep = False)\n",
    "\n",
    "\n",
    "one_attractor_si_diff_no_relativizer, one_attractor_pl_diff_no_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, \"the {} the {}\", first_dep = False)\n",
    "\n",
    "print(\"Error rates different categories: \")\n",
    "\n",
    "error_rate_one_attractor_si_diff_possesive = calculate_error_rate(one_attractor_si_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_possesive , f\"example: {one_attractor_si_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_possesive = calculate_error_rate(one_attractor_pl_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_possesive , f\"example: {one_attractor_pl_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_relativizer = calculate_error_rate(one_attractor_si_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_relativizer , f\"example: {one_attractor_si_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_relativizer = calculate_error_rate(one_attractor_pl_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_relativizer , f\"example: {one_attractor_pl_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_no_relativizer = calculate_error_rate(one_attractor_si_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_no_relativizer , f\"example: {one_attractor_si_diff_no_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_no_relativizer = calculate_error_rate(one_attractor_pl_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_no_relativizer , f\"example: {one_attractor_pl_diff_no_relativizer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(no_attractors_si[0:2])\n",
    "print(no_attractors_pl[0:2])\n",
    "print(one_attractor_si_same[0:2])\n",
    "print(one_attractor_pl_same[0:2])\n",
    "print(one_attractor_si_diff[0:2])\n",
    "print(one_attractor_pl_diff[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2b\n",
    "err_no_attractors_si = calculate_error_rate(no_attractors_si)\n",
    "err_no_attractors_pl = calculate_error_rate(no_attractors_pl)\n",
    "err_one_attractor_si_same =  calculate_error_rate(one_attractor_si_same)\n",
    "err_one_attractor_pl_same = calculate_error_rate(one_attractor_pl_same)\n",
    "err_one_attractor_si_diff = calculate_error_rate(one_attractor_si_diff)\n",
    "err_one_attractor_pl_diff = calculate_error_rate(one_attractor_pl_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproduce plot from the paper 2b\n",
    "N = 2\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.1       # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yvals = [err_no_attractors_pl, err_no_attractors_si]\n",
    "rects1 = ax.bar(ind, yvals, width, color='b')\n",
    "zvals = [err_one_attractor_pl_same, err_one_attractor_si_diff]\n",
    "rects2 = ax.bar(ind+width, zvals, width, color='g')\n",
    "kvals = [err_one_attractor_pl_diff, err_one_attractor_si_same ]\n",
    "rects3 = ax.bar(ind+width*2, kvals, width, color='r')\n",
    "\n",
    "ax.set_ylabel('Error rate')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('Plural subject', 'Singular subject') )\n",
    "ax.set_ylim([0,0.7])\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('None', 'Plural', 'Singular'),title='Last intervening noun')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('2b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error plural subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_pl, err_one_attractor_pl_same,err_one_attractor_pl_diff))\n",
    "print(\"Erro singular subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_si,err_one_attractor_si_diff,err_one_attractor_si_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with 0-4 attractors \n",
    "def gen_num_attractors(num_sentences, num_words, num_attractors, NN, NNS, VBP, VBZ, helper):\n",
    "    assert(num_attractors >= 0)\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences = []\n",
    "    store_indices = []\n",
    "    for i in range(num_sentences):\n",
    "        x,y = np.random.randint(2, size=2)\n",
    "        while True:\n",
    "            indices = np.random.randint(num_words, size=2+num_attractors+1)\n",
    "            if tuple(indices) not in store_indices:\n",
    "                store_indices.append(tuple(indices))\n",
    "                break\n",
    "#  all combinations of homogenous interventions: 2c_mixed.pdf\n",
    "#\n",
    "        if(x == 0):\n",
    "            sent = f\"the {NN[indices[0]]}\"\n",
    "        else:\n",
    "            sent = f\"the {NNS[indices[0]]}\"\n",
    "        for j in range(1,num_attractors+1):\n",
    "            index = np.random.randint(len(helper), size=1)[0]\n",
    "            if(y == 0):\n",
    "                sent += f\" {helper[index]} {NNS[indices[j]]}\"\n",
    "            else:\n",
    "                sent += f\" {helper[index]} {NN[indices[j]]}\"\n",
    "        if(x == 0):\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "        else:\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBP[indices[-1]]))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "no_attr = gen_num_attractors(num_sentences, num_words, 0, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "one_attr = gen_num_attractors(num_sentences, num_words, 1, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "two_attr = gen_num_attractors(num_sentences, num_words, 2, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "three_attr = gen_num_attractors(num_sentences, num_words, 3, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "four_attr = gen_num_attractors(num_sentences, num_words, 4, NN, NNS, VBP, VBZ, attractor_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2c for singular and plural subjects\n",
    "err_no_attr = calculate_error_rate(no_attr)\n",
    "err_one_attr = calculate_error_rate(one_attr)\n",
    "err_two_attr = calculate_error_rate(two_attr)\n",
    "err_three_attr = calculate_error_rate(three_attr)\n",
    "err_four_attr = calculate_error_rate(four_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error for number of attractors: (0, 1, 2, 3, 4) \")\n",
    "print((err_no_attr, err_one_attr, err_two_attr, err_three_attr, err_four_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot error for 2c for singular and plural subjects: 2c_not_mixed.pdf\n",
    "err = [err_no_attr, err_one_attr,err_two_attr,err_three_attr,err_four_attr]\n",
    "\n",
    "x = np.arange(0,5) \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel('Error rate')\n",
    "\n",
    "ax1.set_xlabel('Count of attractors')\n",
    "ax1.plot(x, err, color='g', linewidth=3)\n",
    "ax1.scatter(x,err,color='g',linewidth=4)\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax1.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "ax1.set_ylim([0,0.55])\n",
    "ax1.legend(loc=2)\n",
    "plt.show()\n",
    "fig.savefig('2c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print sentences that incorporate 0-4 homogenous interventions\n",
    "print(no_attr[0:4])\n",
    "print()\n",
    "print(one_attr[0:4])\n",
    "print()\n",
    "print(two_attr[0:4])\n",
    "print()\n",
    "print(three_attr[0:4])\n",
    "print()\n",
    "print(four_attr[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
