\section{Own Experiments}
\label{own-experiments}


\end{multicols}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \caption{}
        \includegraphics[width=\textwidth]{2b_least.pdf}
        \label{fig:2b_least}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}

    \end{subfigure}
    \caption{\ref{fig:2b_least} shows the error rates of the language model for the least frequent nouns in the corpus as the subject and when no intervening nouns are present}
\end{figure}

\begin{multicols}{2}


To predict the correct number of a given verb,
the language model should be able to
1. identify the noun that is the head of the subject for the verb
2. establish the number of the noun (non-trivial since no knowledge of -s postfix for plurals)
3. establish the number of the given verb forms (also non-trivial).

In the first sub section we investigate if our model is able to
do this for simple cases with only a single noun in the prefix.
%
In the second sub section we investigate if our model can handle
more complex cases with two nouns in the prefix,
and what information it then uses to identify the head of the subject.

\subsection{Noun-Verb Agreement in Simple Cases}


In this section we investigate the ability of the model to
establish number agreement for nouns and verbs in the simplest case,
following the pattern: ``The <noun> <verb>''. Notice that
the determiner ``The'' clearly indicates the position of the noun.

1. TEST DATA: 
Generate 100 x 50 simple prefixes in singular and plural form, like ``the company ... [produce/produces]'' 
and ``the companies ... [produce/produces]''.
The sentences do not need to be meaningful.
(randomly pick the nouns and verbs without taking into account their frequency in training data)

2. EXPERIMENT: 
Evaluate how the language model performs on these sentences 
and conclude whether or not the model is sensitive to noun-verb agreement in simple cases. 
(i.e. if it tends to predict plural verbs for plural nouns and singular verbs for singular nouns)
output 1: cross table with correct vs predicted
output 2: overall error rate number, error rate number for plurals, error rate number for singulars 


2. FURTHER ANALYSIS:
- Build a matrix: Nouns x Verbs, 
the entries tell whether the model predicted singular(1)
or plural(0).

- Sort the columns based on their sum
- Sort the rows for singular nouns based on their sum (upper half)
- Sort the rows for plural nouns based on their sum (lower half)
- print as an image (black = plural prediction, white is singular prediction)

- Discuss the picture:
  - Do we see verbs that clearly prefer singular resp. plural?
      (what is their plurality ratio?)
  - Do we see nouns that clearly prefer singular resp. plural
      (i.e. the model established their plurality), 
    or nouns that more or less follow the preference of the verbs?
     what is their frequency (count) in training corpus?)
  - Do we see nouns for which the model thinks
    that they are plural while they are in fact singular?
    
- Optional 1:
  - also include picture which shown models uncertainty in grey teints
    (uncertainty = evaluate(produces)/(evaluate(produce) + evaluate(produces))
    this is instead of max(evaluate(produces), evaluate(produce))

- Optional 2:
  - print noun counts as an y-axis
  - print verb plurality rates as an x-axis
  (both are one dimensional matrices)
  (helps visualize these characteristics)
  (expectation: plurality rates decrease from left to right)
  (expectation: nouns in the middle appear less frequent in training corpus)
  (expectation: more plurality in upper half, resp. singularity in lower half. That is higher error rate)

- Optional 3:
  - repeat experiment for least frequent nouns
  (Is ordering of columns more or less the same?)
  (does the pattern looks different now?)
  
  
- Optional 4:
(probably not in paper)
Show that pattern is not caused by random variations,
i.e. random pattern looks different


%hypothesis: the model falls back to pure frequencies of the two verb forms
%in case it failed to learn the counts because of sparsity in the data.

%Further inspect the error cases, why did it fail for these very simple sentences:
%a) maybe the model failed to learn the number of the noun (i.e. low occurrence in training data for the given noun form)?
%b) maybe the model failed to learn the number of the verb forms (i.e. low occurrence in training data for both verb forms)?
%c) maybe the occurence in the training data of the incorrect but predicted verb form is much higher than the occurrence 
%in training data of the correct form?
%output a: histogram with x-axis z-score noun, y axis count (or percentage) of verbs that fall in this range
%output b: histogram with x-axis z-score verb (max of both), y axis count (or percentage) of verbs that fall in this range
%output c: histogram with x-axis percentage of predicted form, y axis count (or percentage) of verbs that fall in this range
%output(?): a scatter plot, x-axis z-score noun vs y-axis z-score verb, color gradient is percentage of predicted verb form

 
\subsection{Noun-Verb Agreement in Complex Cases}

In Section \ref{replication} we analysed the performance of the model
on complex sentences, containing one or more 
nouns.
The results show that the model is very
sensitive to the most recent noun,
performing worse-than-chance with only one single attractor
(Figure \ref{fig:2c}). 

In this section we investigate whether
syntactic and semantic information
can still help the model 
to establish number agreement
in case of multiple nouns.
We focus on sentences with exactly two nouns
of opposite number.


\subsubsection{Syntactic Information}

%%%%% OBJECTIVE
Function words such as 'that' or 'of' carry 
important information about the syntactic structure of a sentence.
In this Section we investigate if the model
uses this information to establish number agreement
for complex sentences.

%%%% TEST DATA
We generated sets of sentence prefixes using 
different syntactic templates.
An example is:
"The [Noun1] of the [Noun2] ... [VBZ/VBP]".
We instantiate the templates by randomly
picking two nouns and a verb from a set of frequently
used nouns and verbs. 
Each combination of nouns and verbs instantiate
two prefixes that differ by their plurality.
For example:
"The company of the governments ...[know/knows]"
"The companies of the government ...[know/knows]".
%Notice that these prefixes are typically not semantically
%meaningful since the nouns and verbs are randomly picked.

We generated 2 x 1000 sentences per template,
for a total of 11 templates.
The sentences for each template are constructed using the same
noun, verb combinations.
We defined 7 templates for which the first noun is the head of the subject,
while 4 templates have the second noun as the head of the subject.
The templates are shown in figure \ref{x}, respectively figure \ref{y}.

%%%% EXPERIMENT:
We evaluate how the model responds to the generated test inputs.
That is, for each test prefix we let the model decide between 
the singular and plural form of the given verb. 
We measure the error rate for each template.
However, instead of showing the error rates we
show how much the language model tends to agree with the most recent noun.
This correcponds to the error rate for the templates in \ref{x},
while it corresponds to accuracy for the templates in \ref{y}.
Showing the `last noun agreement rate' makes it easier
to compare the behavior of the model for different templates.

%%%% ANALYSIS:
The results are shown in Figure \ref{z},
using green and red colors to indicate if 
the last noun is actually the head of the subject (green)
or not (red). 
%
We see that all bars are above the 0.5 rate,
which shows that the model is most
likely to agree with the last noun,
even in cases where this is syntactically incorrect. 
%
We also see that the red bars are slightly
lower than the green bars on average.
This indicates that the model still has some sensitivity
to syntactic information that points in the direction 
of the first noun as the head of the subject.
%

%
We further discuss two special cases,
namely T1 and T11.
T1 "the \_ and the \_ " is special because it 
actually contains two singular nouns,
instead of one singular and one plural noun. 
The predicted verb should be plural because of the
conjunction word "and".
The two nouns of opposite number make it even harder for the model
to establish number agreement. 
This may explain the high error rate
for T1 (Figure \ref{s}, first red bar).
%
In T11 we used different templates for the singular case
(the \_ 's \_ ) and the plural case (the \_' \_).
The average result is shown in 
Figure \ref{s}, last green bar.
We suspect that the accuracy is relatively
low in this case because the plural possessive form
may not occur frequently. 
Indeed, a closer inspection of the numbers showed that
the singular template had an accuracy
of ..., while the accuracy of the plural template
was considerably lower, ....
%

%%%% DISCUSSION / CONCLUSION:
We conclude that, although the model 
performs bad on complex sentences it
still has some sensitivity for syntactic 
structure exposed by function words. 


\includegraphics[scale=0.5]{screenshot-syntactic-templates} 
%TODO: save picture instead of making screenshot 
%TODO title Templates below
 
 
\begin{tabular}{ l l r }
  T1    & the \_ and the \_     &  0.77 \\
  T2    & the \_ in the \_      &  0.59 \\
  T3    & the \_ by the \_      &  0.69 \\
  T4    & the \_ of the \_      &  0.61 \\
  T5    & the \_ near the \_    &  0.63\\
  T6    & the \_ at the \_      &  0.70\\
  T7    & the \_ without the \_ & 0.67  \\
\end{tabular}

\begin{tabular}{ l l r }
  T8    & the \_ the \_         &  0.78\\
  T9    & the \_ that the \_    &  0.79\\
  T10   & the \_ whether the \_ &  0.85\\
  T11   & the \_ 's \_ (for plural: the \_' \_)          &  0.72 \\
\end{tabular}

\subsubsection{Semantic Information}

We now look at semantic clues, i.e. companies
are more likely to produce than employees.
We investigate if the model is able to establish
number agreement between the verb and the semantic related noun,
ignoring the non-semantic related attractor noun.
  
1. TEST DATA:
We construct a testset (100+ sentences) consisting of pairs with singular and plural prefixes, in the following format  
``The NN of the NNS ...[VBZ/VBP]'' and
``The NNS of the NN ...[VBZ/VBP]'' 
whereby the head of the subject (the first noun)
is semantically related to the verb, while the attractor (the second noun)
is randomly picked. An example is:
``The prices of the employee ...[stabilizes/stabilize]'' and 
``The price of the employees ...[stabilizes/stabilize]''.

In addition we construct a comparison set consisting of the same prefixes
(same verb and attractor noun),
but with the difference that the first noun is also randomly picked and
therefore most likely not semantically related. Example:
``The newspapers of the employee ...[stabilizes/stabilize]'' and 
``The newspaper of the employees ...[stabilizes/stabilize]''.


Remark: the semantically related noun/verb combinations can either be manually
chosen, or preferable, can be learned from the data by looking at verb noun combinations that 'frequently occur together'. 
'frequently occur together' means: count(VBP + NNS)/count(VBP) is 
relatively high.

EXPERIMENT:
Evaluate our model on the constructed testset and on the comparison set.
If the model scores significantly better on the testset
then it shows sensitivity to semantic clues.

%(remark: we can repeat the experiment with the nouns interchanged and check that we %score worse now,
%e.g. "The employee that the prices ... [stabilize/stabilizes]" 
%In that case syntax and semantics are inconsistent)