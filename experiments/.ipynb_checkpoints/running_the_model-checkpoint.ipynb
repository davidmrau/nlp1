{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Grammar in Neural language models\n",
    "\n",
    "We will investigate a language model trained on the penn treebank dataset, using the code provided at https://github.com/pytorch/examples/tree/master/word_language_model. The model consists of an encoder with 2 hidden LSTM layers with 1500 units, and a linear output layer to which a softmax function is applied. The word embeddings have dimensionality 1500. The model is trained for 40 epochs with a dropout factor of 0.65 and has a test perplexity of 72.30 on the test set. If you are interested in more detail in the model, we advise you to look at the repository containing the code.\n",
    "\n",
    "In this notebook, we will walk you through an example of how you can compute the probabilties of the next word in a sentence. You can then use this to start your replication of Linzen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2834)\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib as matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you downloaded and extracted the zipfile, you should have all data required: the model, and the pickled dictionary mapping words to indices.\n",
    "\n",
    "Lets start by loading the model. Because the model was trained on a GPU, we need to specifically say that it should be loaded on the CPU when we load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel (\n",
      "  (drop): Dropout (p = 0.65)\n",
      "  (encoder): Embedding(10000, 1500)\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
      "  (decoder): Linear (1500 -> 10000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the architecture of your model\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a single sentence\n",
    "\n",
    "We will give an example of how you can get the probabilties for the next word in a single sentence. We will uset he example sentence:<br>\n",
    "\n",
    "\"This is a sentence with seven\"\n",
    "\n",
    "And print the probabilities of completing this sentence with either 'words', 'characters', 'thursday', 'days' or 'walk'. As the model itself does not include the mapping from words to indices, we will need to do this as a preprocessing step. The dictionary that maps words to indices is stored in a pickled file called 'dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that can be used to evaluate a single sentence and print the probabilities of finishing this sentence with a word from a list of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    #print (sentence, '\\n')\n",
    "    #print ('\\n'.join(\n",
    "    #        ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "    #        ) )\n",
    "\n",
    "    return {word : get_prob(word) for word in check_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test sentence and words to check\n",
    "test_sentence = 'this is a sentence with seven'\n",
    "check_words = ['words', 'characters', 'thursday', 'days', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': 7.000279583735391e-05,\n",
       " 'days': 0.002636461518704891,\n",
       " 'thursday': 1.165580556516943e-06,\n",
       " 'walk': 5.386583779909415e-06,\n",
       " 'words': 0.0009886504849418998}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it yourself\n",
    "\n",
    "You can now start with your replication of Linzen's paper, for which the first step is to try different inputs with varying distances etc. to get a feeling for what the model is doing and to familiarise yourself with using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are': 0.031047170981764793, 'is': 0.00332761462777853}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"the toy on the tables\"\n",
    "check_words = [\"is\", \"are\"]\n",
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentence, options, correct-option):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the product of the company', ['looks', 'look'], 'looks'),\n",
       " ('the products of the company', ['looks', 'look'], 'look'),\n",
       " ('the product of the companies', ['looks', 'look'], 'looks'),\n",
       " ('the products of the companies', ['looks', 'look'], 'look'),\n",
       " ('the product that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the products that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the product that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the products that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the product that the company produces', ['looks', 'look'], 'looks'),\n",
       " ('the products that the company produces', ['looks', 'look'], 'look'),\n",
       " ('the product that the companies produce', ['looks', 'look'], 'looks'),\n",
       " ('the products that the companies produce', ['looks', 'look'], 'look')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose sentence prefixes with frequent words.\n",
    "# The sentence prefixes are intended to test intervening nouns.\n",
    "\n",
    "\n",
    "NN = ['company', 'year', 'market', 'share', 'stock', 'system', 'president', 'business', \n",
    "      'quarter', 'government', 'time', 'week', 'price', 'group', 'interest',\n",
    "      'industry', 'unit','month', 'rate', 'investment', 'state', 'producer', 'income', \n",
    "      'program', 'bank', 'part', 'plan', 'sale', 'issue', 'tax', 'way', 'loss', 'executive', 'day', 'bid', 'data', 'line','hour', 'plant', 'concern']\n",
    "\n",
    "NNS = ['companies', 'years', 'markets', 'shares', 'stocks', 'systems', 'presidents', \n",
    "       'businesses', 'quarters', 'governments', 'times', 'weeks', 'prices', 'groups', 'interests', 'industries', \n",
    "       'units', 'months', 'rates', 'investments', 'states', 'producers', 'incomes', 'programs', 'banks', 'parts', 'plans', \n",
    "      'sales', 'issues', 'taxes', 'ways', 'losses', 'executives', 'days', 'bids', 'data', 'lines', 'hours', 'plants', 'concerns',]\n",
    "\n",
    "VBP = ['are', 'have', 'do', 'say', 'think', 'want', 'expect', 'include', 'ask', \n",
    "       'make', 'need', 'know', 'see', 'get', 'seem', 'remain', 'continue', 'show', 'buy', \n",
    "       'feel', 'go', 'sell', 'take', 'use', 'plan', 'look', 'tend', 'hope', 'argue', 'give',\n",
    "       'pay', 'appear', 'suggest', 'fear', 'find', 'come', 'offer', 'contend', 'agree', 'provide']\n",
    "\n",
    "VBZ = ['is', 'has', 'does', 'says', 'thinks', 'wants', 'expects', 'includes', 'asks', 'makes',\n",
    "      'needs', 'knows', 'sees', 'gets', 'seems', 'remains', 'continues', 'shows', 'buys', 'feels', 'goes', 'sells',\n",
    "      'takes', 'uses', 'plans', 'looks', 'tends', 'hopes', 'argues', 'gives', 'pays', 'appears', 'suggests', 'fears',\n",
    "      'finds', 'comes', 'offers', 'contends', 'agrees', 'provides']\n",
    "\n",
    "attractor_helpers = ['in the', 'by the', 'close to the', 'of the', 'at the', 'and not the', 'without']\n",
    "\n",
    "words = {\n",
    "    \"NN1\" : \"product\",\n",
    "    \"NNS1\" : \"products\",\n",
    "    \"NN2\" : \"company\",\n",
    "    \"NNS2\" : \"companies\",\n",
    "    \"VBP1\" : \"looks\",\n",
    "    \"VBZ1\" : \"look\",\n",
    "    \"VBP2\" : \"produces\",\n",
    "    \"VBZ2\" : \"produce\",\n",
    "}\n",
    "sentences = [\n",
    "    (f\"the {words['NN1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1'])\n",
    "]\n",
    "\n",
    "print(\"(sentence, options, correct-option):\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_rate(sentences):\n",
    "    result = calculate_errors(sentences)\n",
    "    #print(result)\n",
    "    return 1- sum(result)/len(result)\n",
    "    \n",
    "def calculate_errors(sentences):\n",
    "    return [1 if is_correct_prediction(s[0], s[1], s[2]) else 0 for s in sentences]\n",
    "\n",
    "def is_correct_prediction(sentence, check_words, correct_word):\n",
    "    predictions = evaluate(lm, dictionary, sentence, check_words)\n",
    "    predicted_word = max(predictions, key=predictions.get)\n",
    "    return predicted_word == correct_word\n",
    "    \n",
    "calculate_error_rate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with one and without attractors\n",
    "\n",
    "def gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices_x = []\n",
    "    indices_u = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y = np.random.randint(num_words, size=2)\n",
    "            if (x,y) not in indices_x:\n",
    "                indices_x.append((x,y))\n",
    "                break\n",
    "        while True:\n",
    "            u,v = np.random.randint(num_words, size=2)\n",
    "            if (u,v) not in indices_u:\n",
    "                indices_u.append((u,v))\n",
    "                break\n",
    "        sentences_si.append((f\"the {NN[x]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "        sentences_pl.append((f\"the {NNS[u]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "def gen_one_attractor(num_sentences, num_words, same, NN, NNS, VBP, VBZ, \n",
    "                      template = \"the {} of the {}\", first_dep = True):\n",
    "\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y,z = np.random.randint(num_words, size=3)\n",
    "            if (x,y,z) not in indices:\n",
    "                indices.append((x,y,z))\n",
    "                break\n",
    "        if(same):\n",
    "            sentences_si.append((template.format(NN[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        elif first_dep:\n",
    "            sentences_si.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        else:\n",
    "            sentences_si.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "num_sentences = 1000\n",
    "num_words = len(NN)\n",
    "\n",
    "no_attractors_si, no_attractors_pl = gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_same, one_attractor_pl_same = gen_one_attractor(num_sentences, num_words,True, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_diff, one_attractor_pl_diff = gen_one_attractor(num_sentences, num_words,False, NN, NNS, VBP, VBZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the sale', ['tend', 'tends'], 'tends'), ('the income', ['sell', 'sells'], 'sells')]\n",
      "[('the incomes', ['sell', 'sells'], 'sell'), ('the data', ['go', 'goes'], 'go')]\n",
      "[('the unit of the part', ['think', 'thinks'], 'thinks'), ('the hour of the president', ['give', 'gives'], 'gives')]\n",
      "[('the units of the parts', ['think', 'thinks'], 'think'), ('the hours of the presidents', ['give', 'gives'], 'give')]\n",
      "[('the state of the companies', ['hope', 'hopes'], 'hopes'), ('the business of the markets', ['provide', 'provides'], 'provides')]\n",
      "[('the states of the company', ['hope', 'hopes'], 'hope'), ('the businesses of the market', ['provide', 'provides'], 'provide')]\n"
     ]
    }
   ],
   "source": [
    "print(no_attractors_si[0:2])\n",
    "print(no_attractors_pl[0:2])\n",
    "print(one_attractor_si_same[0:2])\n",
    "print(one_attractor_pl_same[0:2])\n",
    "print(one_attractor_si_diff[0:2])\n",
    "print(one_attractor_pl_diff[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# caluclate error for 2b\n",
    "err_no_attractors_si = calculate_error_rate(no_attractors_si)\n",
    "err_no_attractors_pl = calculate_error_rate(no_attractors_pl)\n",
    "err_one_attractor_si_same =  calculate_error_rate(one_attractor_si_same)\n",
    "err_one_attractor_pl_same = calculate_error_rate(one_attractor_pl_same)\n",
    "err_one_attractor_si_diff = calculate_error_rate(one_attractor_si_diff)\n",
    "err_one_attractor_pl_diff = calculate_error_rate(one_attractor_pl_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEACAYAAAB27puMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1416adb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reproduce plot from the paper 2b\n",
    "N = 2\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.1       # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yvals = [err_no_attractors_pl, err_no_attractors_si]\n",
    "rects1 = ax.bar(ind, yvals, width, color='b')\n",
    "zvals = [err_one_attractor_pl_same, err_one_attractor_si_diff]\n",
    "rects2 = ax.bar(ind+width, zvals, width, color='g')\n",
    "kvals = [err_one_attractor_pl_diff, err_one_attractor_si_same ]\n",
    "rects3 = ax.bar(ind+width*2, kvals, width, color='r')\n",
    "\n",
    "ax.set_ylabel('Error rate')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('Plural subject', 'Singular subject') )\n",
    "ax.set_ylim([0,0.7])\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('None', 'Plural', 'Singular'),title='Last intervening noun')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('2b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error plural subject: (None, Plural, Singular)\n",
      "(0.17700000000000005, 0.18600000000000005, 0.579)\n",
      "Erro singular subject: (None, Plural, Singular)\n",
      "(0.15200000000000002, 0.608, 0.15100000000000002)\n"
     ]
    }
   ],
   "source": [
    "print(\"Error plural subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_pl, err_one_attractor_pl_same,err_one_attractor_pl_diff))\n",
    "print(\"Erro singular subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_si,err_one_attractor_si_diff,err_one_attractor_si_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with 0-4 attractors \n",
    "def gen_num_attractors(num_sentences, num_words, num_attractors, NN, NNS, VBP, VBZ, helper):\n",
    "    assert(num_attractors >= 0)\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences = []\n",
    "    store_indices = []\n",
    "    for i in range(num_sentences):\n",
    "        x = np.random.randint(2, size=1)\n",
    "        while True:\n",
    "            indices = np.random.randint(num_words, size=2+num_attractors+1)\n",
    "            if tuple(indices) not in store_indices:\n",
    "                store_indices.append(tuple(indices))\n",
    "                break\n",
    "#  homogeneous interventions (only consists of agreement attractors)\n",
    "\n",
    "        if(x == 0):\n",
    "            sent = f\"the {NN[indices[0]]}\"\n",
    "        else:\n",
    "            sent = f\"the {NNS[indices[0]]}\"\n",
    "        for j in range(1,num_attractors+1):\n",
    "            index = np.random.randint(len(helper), size=1)[0]\n",
    "            if(x == 0):\n",
    "                sent += f\" {helper[index]} {NNS[indices[j]]}\"\n",
    "            else:\n",
    "                sent += f\" {helper[index]} {NN[indices[j]]}\"\n",
    "        if(x == 0):\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "        else:\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBP[indices[-1]]))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "no_attr = gen_num_attractors(num_sentences, num_words, 0, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "one_attr = gen_num_attractors(num_sentences, num_words, 1, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "two_attr = gen_num_attractors(num_sentences, num_words, 2, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "three_attr = gen_num_attractors(num_sentences, num_words, 3, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "four_attr = gen_num_attractors(num_sentences, num_words, 4, NN, NNS, VBP, VBZ, attractor_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raw_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-29c9af98a7ad>\u001b[0m in \u001b[0;36mtokenise\u001b[0;34m(sentence, dictionary)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ant'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6e50fdfe0f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# caluclate error for 2c for singular and plural subjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0merr_no_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merr_one_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0merr_two_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merr_three_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthree_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-fedb29c0688a>\u001b[0m in \u001b[0;36mcalculate_error_rate\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-fedb29c0688a>\u001b[0m in \u001b[0;36mcalculate_errors\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_correct_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_correct_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-fedb29c0688a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_correct_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_correct_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-fedb29c0688a>\u001b[0m in \u001b[0;36mis_correct_prediction\u001b[0;34m(sentence, check_words, correct_word)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_correct_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpredicted_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_word\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcorrect_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f32d2dd4d7eb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dictionary, sentence, check_words)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# tokenise the sentence, put in torch Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-29c9af98a7ad>\u001b[0m in \u001b[0;36mtokenise\u001b[0;34m(sentence, dictionary)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_input' is not defined"
     ]
    }
   ],
   "source": [
    "# caluclate error for 2c for singular and plural subjects\n",
    "err_no_attr = calculate_error_rate(no_attr)\n",
    "err_one_attr = calculate_error_rate(one_attr)\n",
    "err_two_attr = calculate_error_rate(two_attr)\n",
    "err_three_attr = calculate_error_rate(three_attr)\n",
    "err_four_attr = calculate_error_rate(four_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error for number of attractors: (0, 1, 2, 3, 4) \")\n",
    "print((err_no_attr, err_one_attr, err_two_attr, err_three_attr, err_four_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot error for 2c for singular and plural subjects: 2c_not_mixed.pdf\n",
    "err = [err_no_attr, err_one_attr,err_two_attr,err_three_attr,err_four_attr]\n",
    "\n",
    "x = np.arange(0,5) \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel('Error rate')\n",
    "\n",
    "ax1.set_xlabel('Count of attractors')\n",
    "ax1.plot(x, err, color='g', linewidth=3)\n",
    "ax1.scatter(x,err,color='g',linewidth=4)\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax1.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "ax1.set_ylim([0,0.55])\n",
    "ax1.legend(loc=2)\n",
    "plt.show()\n",
    "fig.savefig('2c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print sentences that incorporate 0-4 homogenous interventions\n",
    "print(no_attr[0:4])\n",
    "print()\n",
    "print(one_attr[0:4])\n",
    "print()\n",
    "print(two_attr[0:4])\n",
    "print()\n",
    "print(three_attr[0:4])\n",
    "print()\n",
    "print(four_attr[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These seed lines make sure that all test variations run on a testset\n",
    "# containing sentences that are composed from the same 'noun'-'verb' combinations.\n",
    "# This has two advantages:\n",
    "# 1) The error rates are more comparable\n",
    "# 2) The outputs can be compared on a one-by-one basis, i.e.\n",
    "# The <keys> that the <cabinet> ...[contain, contains]: 0, \n",
    "# The <keys>      the <cabinet> ...[contain, contains]: 1, \n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_possesive, one_attractor_pl_diff_possesive = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} of the {}\", first_dep = True)\n",
    "\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_relativizer, one_attractor_pl_diff_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} that the {}\", first_dep = False)\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "one_attractor_si_diff_no_relativizer, one_attractor_pl_diff_no_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, \"the {} the {}\", first_dep = False)\n",
    "\n",
    "print(\"Error rates different categories: \")\n",
    "\n",
    "error_rate_one_attractor_si_diff_possesive = calculate_error_rate(one_attractor_si_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_possesive , f\"example: {one_attractor_si_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_possesive = calculate_error_rate(one_attractor_pl_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_possesive , f\"example: {one_attractor_pl_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_relativizer = calculate_error_rate(one_attractor_si_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_relativizer , f\"example: {one_attractor_si_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_relativizer = calculate_error_rate(one_attractor_pl_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_relativizer , f\"example: {one_attractor_pl_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_no_relativizer = calculate_error_rate(one_attractor_si_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_no_relativizer , f\"example: {one_attractor_si_diff_no_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_no_relativizer = calculate_error_rate(one_attractor_pl_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_no_relativizer , f\"example: {one_attractor_pl_diff_no_relativizer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
