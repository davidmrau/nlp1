\section{Related Work}
\label{related work}

%summary of ~\citep{Linzen2016}

%Linzen 2016

%%%% Research Question
LSTM
- can capture long distance statistical regularities
- do not explicitly incorporate syntactic structure
- question: can LSTM capture dependencies that follow from syntactic structure, from a corpus without syntactic annotations?
- more specific: investigate number agreement in English subject-verb dependencies
as an example of a structure sensitive dependency

%%%% Experiments
- train models with different training objectives
  - with explicit grammatical target as training objective
    - number prediction
    - verb infliction
    - grammatical judgement
  - a more generic language model with the objective to predict the next word
- corpus: wikipedia

- evaluate how these models perform on simple and more complex sentences:
  - simple cases: noun that is the subject closeby verb, no intervening nouns
  - effect of long distance: lot of words inbetween noun and verb
  - complex cases: intervening nouns with different number inbetween head of subject noun and verb
  
%%%% Results
- high or reasonable overall accuracy for all models, explained by the fact that most real world sentences are actually simple (no attractors)
   - NP, VI, GJ, LM
- all explicitly trained language models perform reasonable on complex cases.
   - distance
   - attractors
- language model performs bad on complex cases. sensitive to most recent noun.
  - more complex objective, lack of data? no, google also fails.

%%%% Detailed analysis of results
More detailed error analysis shows that
- function words are important, by comparing with a baseline model trained on noun verb sequences
- relative clauses are challenging, especially when relativizer misses
- some errors in identifying nouns (due to ambiguity: drives) and identifying the number of a noun

- activations: units that track main subject, subject of current clause, embedding status, number of main clause subject/most recent noun

%%%% Conclusion
- LSTM can capture grammatical structure given targeted supervision
- language modeling is insufficient for capturing syntax sensitive dependencies
- authors advice: supplement language modeling objectives with more explicit targets
for tasks in which it is desirable to capture syntactic dependencies.