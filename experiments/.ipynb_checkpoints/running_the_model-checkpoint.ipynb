{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Grammar in Neural language models\n",
    "\n",
    "We will investigate a language model trained on the penn treebank dataset, using the code provided at https://github.com/pytorch/examples/tree/master/word_language_model. The model consists of an encoder with 2 hidden LSTM layers with 1500 units, and a linear output layer to which a softmax function is applied. The word embeddings have dimensionality 1500. The model is trained for 40 epochs with a dropout factor of 0.65 and has a test perplexity of 72.30 on the test set. If you are interested in more detail in the model, we advise you to look at the repository containing the code.\n",
    "\n",
    "In this notebook, we will walk you through an example of how you can compute the probabilties of the next word in a sentence. You can then use this to start your replication of Linzen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2834)\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you downloaded and extracted the zipfile, you should have all data required: the model, and the pickled dictionary mapping words to indices.\n",
    "\n",
    "Lets start by loading the model. Because the model was trained on a GPU, we need to specifically say that it should be loaded on the CPU when we load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel (\n",
      "  (drop): Dropout (p = 0.65)\n",
      "  (encoder): Embedding(10000, 1500)\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
      "  (decoder): Linear (1500 -> 10000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the architecture of your model\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a single sentence\n",
    "\n",
    "We will give an example of how you can get the probabilties for the next word in a single sentence. We will uset he example sentence:<br>\n",
    "\n",
    "\"This is a sentence with seven\"\n",
    "\n",
    "And print the probabilities of completing this sentence with either 'words', 'characters', 'thursday', 'days' or 'walk'. As the model itself does not include the mapping from words to indices, we will need to do this as a preprocessing step. The dictionary that maps words to indices is stored in a pickled file called 'dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that can be used to evaluate a single sentence and print the probabilities of finishing this sentence with a word from a list of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    #print (sentence, '\\n')\n",
    "    #print ('\\n'.join(\n",
    "    #        ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "    #        ) )\n",
    "\n",
    "    return {word : get_prob(word) for word in check_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test sentence and words to check\n",
    "test_sentence = 'this is a sentence with seven'\n",
    "check_words = ['words', 'characters', 'thursday', 'days', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': 7.000279583735391e-05,\n",
       " 'days': 0.002636461518704891,\n",
       " 'thursday': 1.165580556516943e-06,\n",
       " 'walk': 5.386583779909415e-06,\n",
       " 'words': 0.0009886504849418998}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it yourself\n",
    "\n",
    "You can now start with your replication of Linzen's paper, for which the first step is to try different inputs with varying distances etc. to get a feeling for what the model is doing and to familiarise yourself with using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are': 0.031047170981764793, 'is': 0.00332761462777853}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"the toy on the tables\"\n",
    "check_words = [\"is\", \"are\"]\n",
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentence, options, correct-option):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the product of the company', ['looks', 'look'], 'looks'),\n",
       " ('the products of the company', ['looks', 'look'], 'look'),\n",
       " ('the product of the companies', ['looks', 'look'], 'looks'),\n",
       " ('the products of the companies', ['looks', 'look'], 'look'),\n",
       " ('the product that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the products that the company', ['produces', 'produce'], 'produces'),\n",
       " ('the product that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the products that the companies', ['produces', 'produce'], 'produce'),\n",
       " ('the product that the company produces', ['looks', 'look'], 'looks'),\n",
       " ('the products that the company produces', ['looks', 'look'], 'look'),\n",
       " ('the product that the companies produce', ['looks', 'look'], 'looks'),\n",
       " ('the products that the companies produce', ['looks', 'look'], 'look')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose sentence prefixes with frequent words.\n",
    "# The sentence prefixes are intended to test intervening nouns.\n",
    "\n",
    "\n",
    "NN = ['company', 'year', 'market', 'share', 'stock', 'system', 'president', 'business', \n",
    "      'quarter', 'government', 'time', 'week', 'price', 'group', 'interest',\n",
    "      'industry', 'unit','month', 'rate', 'investment', 'state', 'producer', 'income', \n",
    "      'program', 'bank', 'part', 'plan', 'sale', 'issue', 'tax', 'way', 'loss', 'executive', 'day', 'bid', 'data', 'line','hour', 'plant', 'concern']\n",
    "\n",
    "NNS = ['companies', 'years', 'markets', 'shares', 'stocks', 'systems', 'presidents', \n",
    "       'businesses', 'quarters', 'governments', 'times', 'weeks', 'prices', 'groups', 'interests', 'industries', \n",
    "       'units', 'months', 'rates', 'investments', 'states', 'producers', 'incomes', 'programs', 'banks', 'parts', 'plans', \n",
    "      'sales', 'issues', 'taxes', 'ways', 'losses', 'executives', 'days', 'bids', 'data', 'lines', 'hours', 'plants', 'concerns',]\n",
    "\n",
    "VBP = ['are', 'have', 'do', 'say', 'think', 'want', 'expect', 'include', 'ask', \n",
    "       'make', 'need', 'know', 'see', 'get', 'seem', 'remain', 'continue', 'show', 'buy', \n",
    "       'feel', 'go', 'sell', 'take', 'use', 'plan', 'look', 'tend', 'hope', 'argue', 'give',\n",
    "       'pay', 'appear', 'suggest', 'fear', 'find', 'come', 'offer', 'contend', 'agree', 'provide']\n",
    "\n",
    "VBZ = ['is', 'has', 'does', 'says', 'thinks', 'wants', 'expects', 'includes', 'asks', 'makes',\n",
    "      'needs', 'knows', 'sees', 'gets', 'seems', 'remains', 'continues', 'shows', 'buys', 'feels', 'goes', 'sells',\n",
    "      'takes', 'uses', 'plans', 'looks', 'tends', 'hopes', 'argues', 'gives', 'pays', 'appears', 'suggests', 'fears',\n",
    "      'finds', 'comes', 'offers', 'contends', 'agrees', 'provides']\n",
    "\n",
    "attractor_helpers = ['in the', 'by the', 'close to the', 'of the', 'at the', 'ant not the', 'without']\n",
    "\n",
    "words = {\n",
    "    \"NN1\" : \"product\",\n",
    "    \"NNS1\" : \"products\",\n",
    "    \"NN2\" : \"company\",\n",
    "    \"NNS2\" : \"companies\",\n",
    "    \"VBP1\" : \"looks\",\n",
    "    \"VBZ1\" : \"look\",\n",
    "    \"VBP2\" : \"produces\",\n",
    "    \"VBZ2\" : \"produce\",\n",
    "}\n",
    "sentences = [\n",
    "    (f\"the {words['NN1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NN2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} of the {words['NNS2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']}\", [words['VBP2'], words['VBZ2']], words['VBP2']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']}\", [words['VBP2'], words['VBZ2']], words['VBZ2']),\n",
    "\n",
    "    (f\"the {words['NN1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NN2']} {words['VBP2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1']),\n",
    "    (f\"the {words['NN1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBP1']),\n",
    "    (f\"the {words['NNS1']} that the {words['NNS2']} {words['VBZ2']}\", [words['VBP1'], words['VBZ1']], words['VBZ1'])\n",
    "]\n",
    "\n",
    "print(\"(sentence, options, correct-option):\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_rate(sentences):\n",
    "    result = [1 if is_correct_prediction(s[0], s[1], s[2]) else 0 for s in sentences]\n",
    "    #print(result)\n",
    "    return 1- sum(result)/len(result)\n",
    "    \n",
    "\n",
    "def is_correct_prediction(sentence, check_words, correct_word):\n",
    "    predictions = evaluate(lm, dictionary, sentence, check_words)\n",
    "    predicted_word = max(predictions, key=predictions.get)\n",
    "    return predicted_word == correct_word\n",
    "    \n",
    "is_correct_prediction(sentences[0][0], sentences[0][1], sentences[0][2])\n",
    "calculate_error_rate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with one and without attractors\n",
    "\n",
    "def gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y = np.random.randint(num_words, size=2)\n",
    "            if (x,y) not in indices:\n",
    "                indices.append((x,y))\n",
    "                break\n",
    "        sentences_si.append((f\"the {NN[x]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "        sentences_pl.append((f\"the {NNS[x]}\", [VBP[y], VBZ[y]], VBP[y],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "def gen_one_attractor(num_sentences, num_words,same,NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y,z = np.random.randint(num_words, size=3)\n",
    "            if (x,y,z) not in indices:\n",
    "                indices.append((x,y,z))\n",
    "                break\n",
    "        if(same):\n",
    "            sentences_si.append((f\"the {NN[x]} of the {NN[z]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((f\"the {NNS[x]} of the {NNS[z]}\", [VBP[y], VBZ[y]], VBP[y],))\n",
    "        else:\n",
    "            sentences_si.append((f\"the {NN[x]} of the {NNS[z]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((f\"the {NNS[x]} of the {NN[z]}\", [VBP[y], VBZ[y]], VBP[y],))\n",
    "            \n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "num_sentences = 100\n",
    "num_words = len(NN)\n",
    "\n",
    "no_attractors_si, no_attractors_pl = gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_same, one_attractor_pl_same = gen_one_attractor(num_sentences, num_words,True, NN, NNS, VBP, VBZ)\n",
    "one_attractor_si_diff, one_attractor_pl_diff = gen_one_attractor(num_sentences, num_words,False, NN, NNS, VBP, VBZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2b\n",
    "err_no_attractors_si = calculate_error_rate(no_attractors_si)\n",
    "err_no_attractors_pl = calculate_error_rate(no_attractors_pl)\n",
    "err_one_attractor_si_same =  calculate_error_rate(one_attractor_si_same)\n",
    "err_one_attractor_pl_same = calculate_error_rate(one_attractor_pl_same)\n",
    "err_one_attractor_si_diff = calculate_error_rate(one_attractor_si_diff)\n",
    "err_one_attractor_pl_diff = calculate_error_rate(one_attractor_pl_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11de0b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reproduce plot from the paper 2b\n",
    "N = 2\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.1       # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yvals = [err_no_attractors_pl, err_no_attractors_si]\n",
    "rects1 = ax.bar(ind, yvals, width, color='b')\n",
    "zvals = [err_one_attractor_pl_same, err_one_attractor_si_diff]\n",
    "rects2 = ax.bar(ind+width, zvals, width, color='g')\n",
    "kvals = [err_one_attractor_pl_diff, err_one_attractor_si_same ]\n",
    "rects3 = ax.bar(ind+width*2, kvals, width, color='r')\n",
    "\n",
    "ax.set_ylabel('Error rate')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('Plural subject', 'Singular subject') )\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0]), ('None', 'Plural', 'Singular') )\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.show()\n",
    "fig.savefig('2b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error plural subject: (None, Plural, Singular)\n",
      "(0.17700000000000005, 0.17500000000000004, 0.614)\n",
      "Erro singular subject: (None, Plural, Singular)\n",
      "(0.16000000000000003, 0.5840000000000001, 0.15000000000000002)\n"
     ]
    }
   ],
   "source": [
    "print(\"Error plural subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_pl, err_one_attractor_pl_same,err_one_attractor_pl_diff))\n",
    "print(\"Erro singular subject: (None, Plural, Singular)\")\n",
    "print((err_no_attractors_si,err_one_attractor_si_diff,err_one_attractor_si_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with 0-4 attractors \n",
    "def gen_num_attractors(num_sentences, num_words, num_attractors, NN, NNS, VBP, VBZ, helper):\n",
    "    assert(num_attractors >= 0)\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    x,y = np.random.randint(2, size=2)\n",
    "    sentences = []\n",
    "    store_indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            indices = np.random.randint(num_words, size=2+num_attractors+1)\n",
    "            if tuple(indices) not in store_indices:\n",
    "                store_indices.append(tuple(indices))\n",
    "                break\n",
    "#  all combinations of homogenous interventions gives strange results see: 2c_mixed.pdf\n",
    "#\n",
    "#         if(x == 0):\n",
    "#             sent = f\"the {NN[indices[0]]}\"\n",
    "#         else:\n",
    "#             sent = f\"the {NNS[indices[0]]}\"\n",
    "#         for j in range(1,num_attractors+1):\n",
    "#             index = np.random.randint(len(helper), size=1)[0]\n",
    "#             if(y == 0):\n",
    "#                 sent += f\" {helper[index]} {NNS[indices[j]]}\"\n",
    "#             else:\n",
    "#                 sent += f\" {helper[index]} {NN[indices[j]]}\"\n",
    "#         if(x == 0):\n",
    "#             sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "#         else:\n",
    "#             sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBP[indices[-1]]))\n",
    "\n",
    "# just homogenous interventions where the attractor has a different plrality than the subject\n",
    "        if(x == 0):\n",
    "            sent = f\"the {NN[indices[0]]}\"\n",
    "            for j in range(1,num_attractors+1):\n",
    "                index = np.random.randint(len(helper), size=1)[0]\n",
    "                sent += f\" {helper[index]} {NNS[indices[j]]}\"\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "        else:\n",
    "            sent = f\"the {NNS[indices[0]]}\"\n",
    "            for j in range(1,num_attractors+1):\n",
    "                index = np.random.randint(len(helper), size=1)[0]\n",
    "                sent += f\" {helper[index]} {NN[indices[j]]}\"\n",
    "            sentences.append((sent, [VBP[indices[-1]], VBZ[indices[-1]]], VBZ[indices[-1]]))\n",
    "    return sentences\n",
    "\n",
    "no_attr = gen_num_attractors(num_sentences, num_words, 0, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "one_attr = gen_num_attractors(num_sentences, num_words, 1, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "two_attr = gen_num_attractors(num_sentences, num_words, 2, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "three_attr = gen_num_attractors(num_sentences, num_words, 3, NN, NNS, VBP, VBZ, attractor_helpers)\n",
    "four_attr = gen_num_attractors(num_sentences, num_words, 4, NN, NNS, VBP, VBZ, attractor_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate error for 2c for singular and plural subjects\n",
    "err_no_attr = calculate_error_rate(no_attr)\n",
    "err_one_attr = calculate_error_rate(one_attr)\n",
    "err_two_attr = calculate_error_rate(two_attr)\n",
    "err_three_attr = calculate_error_rate(three_attr)\n",
    "err_four_attr = calculate_error_rate(four_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error number of attractors: (0, 1, 2, 3, 4) \")\n",
    "print((err_no_attr, err_one_attr, err_two_attr, err_three_attr, err_four_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error for 2c for singular and plural subjects\n",
    "err = [err_no_attr, err_one_attr,err_two_attr,err_three_attr,err_four_attr]\n",
    "\n",
    "x = np.arange(0,5) \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel('Error rate')\n",
    "ax1.set_xlabel('Count of attractors')\n",
    "ax1.plot(x, err, color='g')\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax1.xaxis.set_major_locator(mtick.MaxNLocator(integer=True))\n",
    "ax1.set_ylim([0,0.75])\n",
    "ax1.legend(loc=2)\n",
    "plt.show()\n",
    "fig.savefig('2c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the interests', ['offer', 'offers'], 'offer'), ('the rates', ['include', 'includes'], 'include'), ('the industries', ['sell', 'sells'], 'sell'), ('the stocks', ['suggest', 'suggests'], 'suggest')]\n",
      "\n",
      "[('the government close to the groups', ['do', 'does'], 'does'), ('the plan of the hours', ['remain', 'remains'], 'remains'), ('the company by the quarters', ['come', 'comes'], 'comes'), ('the line in the weeks', ['agree', 'agrees'], 'agrees')]\n",
      "\n",
      "[('the states by the producer by the bid', ['know', 'knows'], 'know'), ('the shares close to the month of the data', ['say', 'says'], 'say'), ('the weeks of the interest by the business', ['say', 'says'], 'say'), ('the businesses in the industry close to the price', ['find', 'finds'], 'find')]\n",
      "\n",
      "[('the issues close to the sales by the prices close to the losses', ['ask', 'asks'], 'ask'), ('the data of the hours of the days of the plants', ['find', 'finds'], 'find'), ('the lines in the plants close to the lines of the governments', ['want', 'wants'], 'want'), ('the incomes close to the ways in the companies close to the programs', ['make', 'makes'], 'make')]\n",
      "\n",
      "[('the interest by the part by the bid close to the year by the issue', ['see', 'sees'], 'sees'), ('the way by the time close to the stock in the rate in the loss', ['feel', 'feels'], 'feels'), ('the income of the rate by the income of the plan by the interest', ['agree', 'agrees'], 'agrees'), ('the government of the group close to the group in the data close to the tax', ['know', 'knows'], 'knows')]\n"
     ]
    }
   ],
   "source": [
    "print(no_attr[0:4])\n",
    "print()\n",
    "print(one_attr[0:4])\n",
    "print()\n",
    "print(two_attr[0:4])\n",
    "print()\n",
    "print(three_attr[0:4])\n",
    "print()\n",
    "print(four_attr[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
