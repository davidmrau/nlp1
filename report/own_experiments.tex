\section{Own Experiments}
\label{own-experiments}


\end{multicols}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    \caption{}
        \includegraphics[width=\textwidth]{2b_least.pdf}
        \label{fig:2b_least}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}

    \end{subfigure}
    \caption{\ref{fig:2b_least} shows the error rates of the language model for the least frequent nouns in the corpus as the subject and when no intervening nouns are present}
\end{figure}

\begin{multicols}{2}


To predict the correct number of a given verb,
the language model should be able to
1. identify the noun that is the head of the subject for the verb
2. establish the number of the noun (non-trivial since no knowledge of -s postfix for plurals)
3. establish the number of the given verb forms (also non-trivial).

In the first sub section we investigate if our model is able to
do this for simple cases with only a single noun in the prefix.
%
In the second sub section we investigate if our model can handle
more complex cases with two nouns in the prefix,
and what information it then uses to identify the head of the subject.

\subsection{Noun-Verb Agreement in Simple Cases}


In this section we investigate the ability of the model to
establish number agreement for nouns and verbs in the simplest case,
following the pattern: ``The <noun> <verb>''. Notice that
the determiner ``The'' clearly indicates the position of the noun.

1. TEST DATA: 
Generate 100 x 50 simple prefixes in singular and plural form, like ``the company ... [produce/produces]'' 
and ``the companies ... [produce/produces]''.
The sentences do not need to be meaningful.
(randomly pick the nouns and verbs without taking into account their frequency in training data)

2. EXPERIMENT: 
Evaluate how the language model performs on these sentences 
and conclude whether or not the model is sensitive to noun-verb agreement in simple cases. 
(i.e. if it tends to predict plural verbs for plural nouns and singular verbs for singular nouns)
output 1: cross table with correct vs predicted
output 2: overall error rate number, error rate number for plurals, error rate number for singulars 


2. FURTHER ANALYSIS:
- Build a matrix: Nouns x Verbs, 
the entries tell whether the model predicted singular(1)
or plural(0).

- Sort the columns based on their sum
- Sort the rows for singular nouns based on their sum (upper half)
- Sort the rows for plural nouns based on their sum (lower half)
- print as an image (black = plural prediction, white is singular prediction)

- Discuss the picture:
  - Do we see verbs that clearly prefer singular resp. plural?
      (what is their plurality ratio?)
  - Do we see nouns that clearly prefer singular resp. plural
      (i.e. the model established their plurality), 
    or nouns that more or less follow the preference of the verbs?
     what is their frequency (count) in training corpus?)
  - Do we see nouns for which the model thinks
    that they are plural while they are in fact singular?
    
- Optional 1:
  - also include picture which shown models uncertainty in grey teints
    (uncertainty = evaluate(produces)/(evaluate(produce) + evaluate(produces))
    this is instead of max(evaluate(produces), evaluate(produce))

- Optional 2:
  - print noun counts as an y-axis
  - print verb plurality rates as an x-axis
  (both are one dimensional matrices)
  (helps visualize these characteristics)
  (expectation: plurality rates decrease from left to right)
  (expectation: nouns in the middle appear less frequent in training corpus)
  (expectation: more plurality in upper half, resp. singularity in lower half. That is higher error rate)

- Optional 3:
  - repeat experiment for least frequent nouns
  (Is ordering of columns more or less the same?)
  (does the pattern looks different now?)
  
  
- Optional 4:
(probably not in paper)
Show that pattern is not caused by random variations,
i.e. random pattern looks different


%hypothesis: the model falls back to pure frequencies of the two verb forms
%in case it failed to learn the counts because of sparsity in the data.

%Further inspect the error cases, why did it fail for these very simple sentences:
%a) maybe the model failed to learn the number of the noun (i.e. low occurrence in training data for the given noun form)?
%b) maybe the model failed to learn the number of the verb forms (i.e. low occurrence in training data for both verb forms)?
%c) maybe the occurence in the training data of the incorrect but predicted verb form is much higher than the occurrence 
%in training data of the correct form?
%output a: histogram with x-axis z-score noun, y axis count (or percentage) of verbs that fall in this range
%output b: histogram with x-axis z-score verb (max of both), y axis count (or percentage) of verbs that fall in this range
%output c: histogram with x-axis percentage of predicted form, y axis count (or percentage) of verbs that fall in this range
%output(?): a scatter plot, x-axis z-score noun vs y-axis z-score verb, color gradient is percentage of predicted verb form

 
\subsection{Noun-Verb Agreement in Complex Cases}

- complex cases: intervening nouns of opposite number in the prefix
- confused, below random guess 
- model sensitive to most recent noun \label{fig:2c}
- in this section: 
investigate if syntactic and semantic information
can help the model to establish number agreement
in case of attractors.
We focus on sentences with exactly one attractor.
 

Figure \label{fig:2c} 

When multiple nouns in the prefix, how does the model 'pick' the noun to agree with?
Most recent noun is a simple heuristic, but maybe it can do better based
on the available information?

\subsubsection{Syntactic Information}

We first look at syntactic clues such as punctuation
and function words such as 'that' or 'of'
that indicate the start of a relative resp. possesive
clause. Do these words help the model to pick
the noun that serves as the head of the sentence?

1. TEST DATA:
Generate sentences using different templates, singular and plural formats:
"The [Noun1] of the [Noun2] ..."
"The [Noun1] and the [Noun2] ..."
"The [Noun1] that the [Noun2] ..."
"The [Noun1] , that the [Noun2] ..."
"The [Noun1] the [Noun2] ..."
...
Discuss templates and function words.
[Remark: pick noun/verb combinations for which the simple cases succeed?!]

2. EXPERIMENT:
Evaluate how much the language model tends to agree with the last noun 
(correct for some templates, erroneous for others)
output: bar chart with template-id on x axis, percentage of agreement
with last noun on y axis, color green if correct, color red if erroneous.  

3. ANALYSIS:
The model is less likely to agree with the number of the 
last noun in case it has syntactic information
that points in the direction of the first noun as the head of the subject.
(green bars are significantly higher than red bars,
templates with relativizer scores better than without)
Is this indeed the case ???

\includegraphics[scale=0.5]{screenshot-syntactic-templates} 
%TODO: save picture instead of making screenshot 
%TODO title Templates below
 
 
\begin{tabular}{ l l r }
  T1    & the \_ and the \_     &  0.77 \\
  T2    & the \_ in the \_      &  0.59 \\
  T3    & the \_ by the \_      &  0.69 \\
  T4    & the \_ of the \_      &  0.61 \\
  T5    & the \_ near the \_    &  0.63\\
  T6    & the \_ at the \_      &  0.70\\
  T7    & the \_ without the \_ & 0.67  \\
  T8    & the \_ the \_         &  0.78\\
  T9    & the \_ that the \_    &  0.79\\
  T10   & the \_ whether the \_ &  0.85\\
  T11   & the \_ 's \_ (for plural: the \_' \_)          &  0.72 \\
\end{tabular}

\subsubsection{Semantic Information}

We now look at semantic clues, i.e. companies
are more likely to produce than employees.
We investigate if the model is able to establish
number agreement between the verb and the semantic related noun,
ignoring the non-semantic related attractor noun.
  
1. TEST DATA:
We construct a testset (100+ sentences) consisting of pairs with singular and plural prefixes, in the following format  
``The NN of the NNS ...[VBZ/VBP]'' and
``The NNS of the NN ...[VBZ/VBP]'' 
whereby the head of the subject (the first noun)
is semantically related to the verb, while the attractor (the second noun)
is randomly picked. An example is:
``The prices of the employee ...[stabilizes/stabilize]'' and 
``The price of the employees ...[stabilizes/stabilize]''.

In addition we construct a comparison set consisting of the same prefixes
(same verb and attractor noun),
but with the difference that the first noun is also randomly picked and
therefore most likely not semantically related. Example:
``The newspapers of the employee ...[stabilizes/stabilize]'' and 
``The newspaper of the employees ...[stabilizes/stabilize]''.


Remark: the semantically related noun/verb combinations can either be manually
chosen, or preferable, can be learned from the data by looking at verb noun combinations that 'frequently occur together'. 
'frequently occur together' means: count(VBP + NNS)/count(VBP) is 
relatively high.

EXPERIMENT:
Evaluate our model on the constructed testset and on the comparison set.
If the model scores significantly better on the testset
then it shows sensitivity to semantic clues.

%(remark: we can repeat the experiment with the nouns interchanged and check that we %score worse now,
%e.g. "The employee that the prices ... [stabilize/stabilizes]" 
%In that case syntax and semantics are inconsistent)