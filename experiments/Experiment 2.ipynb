{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import difflib\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2834)\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib as matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    #print (sentence, '\\n')\n",
    "    #print ('\\n'.join(\n",
    "    #        ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "    #        ) )\n",
    "    return  [{word : get_prob(word)} for word in check_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words.\n",
    "# The sentence prefixes are intended to test intervening nouns.\n",
    "\n",
    "\n",
    "NN = ['company', 'year', 'market', 'share', 'stock', 'system', 'president', 'business', \n",
    "      'quarter', 'government', 'time', 'week', 'price', 'group', 'interest',\n",
    "      'industry', 'unit','month', 'rate', 'investment', 'state', 'producer', 'income', \n",
    "      'program', 'bank', 'part', 'plan', 'sale', 'issue', 'tax', 'way', 'loss', 'executive', 'day', 'bid', 'data', 'line','hour', 'plant', 'concern']\n",
    "\n",
    "NNS = ['companies', 'years', 'markets', 'shares', 'stocks', 'systems', 'presidents', \n",
    "       'businesses', 'quarters', 'governments', 'times', 'weeks', 'prices', 'groups', 'interests', 'industries', \n",
    "       'units', 'months', 'rates', 'investments', 'states', 'producers', 'incomes', 'programs', 'banks', 'parts', 'plans', \n",
    "      'sales', 'issues', 'taxes', 'ways', 'losses', 'executives', 'days', 'bids', 'data', 'lines', 'hours', 'plants', 'concerns',]\n",
    "\n",
    "VBP = ['are', 'have', 'do', 'say', 'think', 'want', 'expect', 'include', 'ask', \n",
    "       'make', 'need', 'know', 'see', 'get', 'seem', 'remain', 'continue', 'show', 'buy', \n",
    "       'feel', 'go', 'sell', 'take', 'use', 'plan', 'look', 'tend', 'hope', 'argue', 'give',\n",
    "       'pay', 'appear', 'suggest', 'fear', 'find', 'come', 'offer', 'contend', 'agree', 'provide']\n",
    "\n",
    "VBZ = ['is', 'has', 'does', 'says', 'thinks', 'wants', 'expects', 'includes', 'asks', 'makes',\n",
    "      'needs', 'knows', 'sees', 'gets', 'seems', 'remains', 'continues', 'shows', 'buys', 'feels', 'goes', 'sells',\n",
    "      'takes', 'uses', 'plans', 'looks', 'tends', 'hopes', 'argues', 'gives', 'pays', 'appears', 'suggests', 'fears',\n",
    "      'finds', 'comes', 'offers', 'contends', 'agrees', 'provides']\n",
    "\n",
    "attractor_helpers = ['in the', 'by the', 'close to the', 'of the', 'at the', 'and not the', 'without']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_rate(sentences):\n",
    "    result = calculate_errors(sentences)\n",
    "    #print(result)\n",
    "    return 1- sum(result)/len(result)\n",
    "    \n",
    "def calculate_errors(sentences):\n",
    "    return [1 if is_correct_prediction(s[0], s[1], s[2]) else 0 for s in sentences]\n",
    "\n",
    "def is_correct_prediction(sentence, check_words, correct_word):\n",
    "    predictions = evaluate(lm, dictionary, sentence, check_words)\n",
    "    words,preds = zip(*list(map(lambda x: list(x.items())[0],predictions)))\n",
    "    predicted_word = words[np.argmax(preds)]\n",
    "    return predicted_word == correct_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compose sentence prefixes with frequent words with one and without attractors\n",
    "\n",
    "def gen_no_attractors(num_sentences, num_words, NN, NNS, VBP, VBZ):\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices_x = []\n",
    "    indices_u = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y = np.random.randint(num_words, size=2)\n",
    "            if (x,y) not in indices_x:\n",
    "                indices_x.append((x,y))\n",
    "                break\n",
    "        while True:\n",
    "            u,v = np.random.randint(num_words, size=2)\n",
    "            if (u,v) not in indices_u:\n",
    "                indices_u.append((u,v))\n",
    "                break\n",
    "        sentences_si.append((f\"the {NN[x]}\", [VBP[y], VBZ[y]], VBZ[y],))\n",
    "        sentences_pl.append((f\"the {NNS[u]}\", [VBP[v], VBZ[v]], VBP[v],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "def gen_one_attractor(num_sentences, num_words, same, NN, NNS, VBP, VBZ, \n",
    "                      template = \"the {} of the {}\", first_dep = True):\n",
    "\n",
    "    assert(len(NN) == len(NNS) == len(VBP) == len(VBZ))\n",
    "    sentences_si = []\n",
    "    sentences_pl =[]\n",
    "    indices = []\n",
    "    for i in range(num_sentences):\n",
    "        while True:\n",
    "            x,y,z = np.random.randint(num_words, size=3)\n",
    "            if (x,y,z) not in indices:\n",
    "                indices.append((x,y,z))\n",
    "                break\n",
    "        if(same):\n",
    "            sentences_si.append((template.format(NN[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        elif first_dep:\n",
    "            sentences_si.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "        else:\n",
    "            sentences_si.append((template.format(NNS[x], NN[z]), [VBP[y], VBZ[y]], VBZ[y],))\n",
    "            sentences_pl.append((template.format(NN[x], NNS[z]), [VBP[y], VBZ[y]], VBP[y],))\n",
    "    return sentences_si, sentences_pl\n",
    "\n",
    "num_sentences = 1000\n",
    "num_words = len(NN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates different categories: \n",
      "0.62 example: ('the quarter of the shares', ['plan', 'plans'], 'plans')\n",
      "0.65 example: ('the quarters of the share', ['plan', 'plans'], 'plan')\n",
      "0.20 example: ('the quarters that the share', ['plan', 'plans'], 'plans')\n",
      "0.29 example: ('the quarter that the shares', ['plan', 'plans'], 'plan')\n",
      "0.17 example: ('the quarters the share', ['plan', 'plans'], 'plans')\n",
      "0.27 example: ('the quarter the shares', ['plan', 'plans'], 'plan')\n"
     ]
    }
   ],
   "source": [
    "# These seed lines make sure that all test variations run on a testset\n",
    "# containing sentences that are composed from the same 'noun'-'verb' combinations.\n",
    "# This has two advantages:\n",
    "# 1) The error rates are more comparable\n",
    "# 2) The outputs can be compared on a one-by-one basis, i.e.\n",
    "# The <keys> that the <cabinet> ...[contain, contains]: 0, \n",
    "# The <keys>      the <cabinet> ...[contain, contains]: 1, \n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_possesive, one_attractor_pl_diff_possesive = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} of the {}\", first_dep = True)\n",
    "\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "#compare different templates\n",
    "#calculate_errors(one_attractor_si_same[0:100])\n",
    "one_attractor_si_diff_relativizer, one_attractor_pl_diff_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, template = \"the {} that the {}\", first_dep = False)\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "one_attractor_si_diff_no_relativizer, one_attractor_pl_diff_no_relativizer = gen_one_attractor(\n",
    "    num_sentences, num_words,False, NN, NNS, VBP, VBZ, \"the {} the {}\", first_dep = False)\n",
    "\n",
    "print(\"Error rates different categories: \")\n",
    "\n",
    "error_rate_one_attractor_si_diff_possesive = calculate_error_rate(one_attractor_si_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_possesive , f\"example: {one_attractor_si_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_possesive = calculate_error_rate(one_attractor_pl_diff_possesive[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_possesive , f\"example: {one_attractor_pl_diff_possesive[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_relativizer = calculate_error_rate(one_attractor_si_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_relativizer , f\"example: {one_attractor_si_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_relativizer = calculate_error_rate(one_attractor_pl_diff_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_relativizer , f\"example: {one_attractor_pl_diff_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_si_diff_no_relativizer = calculate_error_rate(one_attractor_si_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_si_diff_no_relativizer , f\"example: {one_attractor_si_diff_no_relativizer[0]}\")\n",
    "\n",
    "error_rate_one_attractor_pl_diff_no_relativizer = calculate_error_rate(one_attractor_pl_diff_no_relativizer[0:100])\n",
    "print (\"%.2f\" % error_rate_one_attractor_pl_diff_no_relativizer , f\"example: {one_attractor_pl_diff_no_relativizer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_attractors_si_least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"the {} of the {}\",\n",
    "    \"the {} in the {}\",\n",
    "    \"the {} the {}\",\n",
    "    \"the {} that the {}\"\n",
    "#    \"the {} , that the {}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the {} of the {}\n",
      "the {} in the {}\n",
      "the {} the {}\n",
      "the {} that the {}\n",
      "{'the {} of the {}': 0.635, 'the {} in the {}': 0.595, 'the {} the {}': 0.78, 'the {} that the {}': 0.755}\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 100\n",
    "\n",
    "results = {}\n",
    "for t in templates:\n",
    "    np.random.seed(100) #generate from same verbs and nouns for each template\n",
    "    print (t)\n",
    "    si, pl = gen_one_attractor(\n",
    "        num_sentences, \n",
    "        num_words, \n",
    "        False, \n",
    "        NN, \n",
    "        NNS, \n",
    "        VBP, \n",
    "        VBZ, \n",
    "        template = t, \n",
    "        first_dep = True)\n",
    "    si_most_recent_noun_rate = calculate_error_rate(si)\n",
    "    pl_most_recent_noun_rate = calculate_error_rate(pl)\n",
    "    most_recent_noun_rate = 0.5*(pl_most_recent_noun_rate + si_most_recent_noun_rate)\n",
    "    results[t] = most_recent_noun_rate\n",
    "\n",
    "\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEMCAYAAAA1VZrrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w/akfWNG3Z50gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6ccf86d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template_names = [f\"T{i + 1}\" for i in range(0, len(templates))]\n",
    "last_noun_rates = results.values()\n",
    "y_pos = np.arange(len(templates))\n",
    "plt.bar(y_pos, last_noun_rates, align='center', color=['r', 'r', 'g', 'g'])\n",
    "plt.xticks(y_pos, template_names)\n",
    "plt.ylabel('Last Noun Preference Rate')\n",
    "plt.title('Templates')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template = \"the {} and the {}\"\n",
    "\n",
    "si, pl = gen_one_attractor(\n",
    "    num_sentences, \n",
    "    num_words, \n",
    "    True, \n",
    "    NN, \n",
    "    NNS, \n",
    "    VBP, \n",
    "    VBZ, \n",
    "    template = template)\n",
    "\n",
    "si_error_rate = 1 - calculate_error_rate(si) #error means plural prediction which is actually good \n",
    "pl_error_rate = calculate_error_rate(pl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the loss and the plan', ['take', 'takes'], 'takes'),\n",
       " ('the issue and the tax', ['show', 'shows'], 'shows'),\n",
       " ('the week and the part', ['agree', 'agrees'], 'agrees'),\n",
       " ('the data and the rate', ['have', 'has'], 'has'),\n",
       " ('the plan and the loss', ['include', 'includes'], 'includes'),\n",
       " ('the price and the data', ['take', 'takes'], 'takes'),\n",
       " ('the state and the market', ['seem', 'seems'], 'seems'),\n",
       " ('the program and the executive', ['sell', 'sells'], 'sells'),\n",
       " ('the unit and the interest', ['are', 'is'], 'is'),\n",
       " ('the month and the stock', ['show', 'shows'], 'shows')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
