{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Grammar in Neural language models\n",
    "\n",
    "We will investigate a language model trained on the penn treebank dataset, using the code provided at https://github.com/pytorch/examples/tree/master/word_language_model. The model consists of an encoder with 2 hidden LSTM layers with 1500 units, and a linear output layer to which a softmax function is applied. The word embeddings have dimensionality 1500. The model is trained for 40 epochs with a dropout factor of 0.65 and has a test perplexity of 72.30 on the test set. If you are interested in more detail in the model, we advise you to look at the repository containing the code.\n",
    "\n",
    "In this notebook, we will walk you through an example of how you can compute the probabilties of the next word in a sentence. You can then use this to start your replication of Linzen et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do required imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you downloaded and extracted the zipfile, you should have all data required: the model, and the pickled dictionary mapping words to indices.\n",
    "\n",
    "Lets start by loading the model. Because the model was trained on a GPU, we need to specifically say that it should be loaded on the CPU when we load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = torch.load('model.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel (\n",
      "  (drop): Dropout (p = 0.65)\n",
      "  (encoder): Embedding(10000, 1500)\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
      "  (decoder): Linear (1500 -> 10000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print a summary of the architecture of your model\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a single sentence\n",
    "\n",
    "We will give an example of how you can get the probabilties for the next word in a single sentence. We will uset he example sentence:<br>\n",
    "\n",
    "\"This is a sentence with seven\"\n",
    "\n",
    "And print the probabilities of completing this sentence with either 'words', 'characters', 'thursday', 'days' or 'walk'. As the model itself does not include the mapping from words to indices, we will need to do this as a preprocessing step. The dictionary that maps words to indices is stored in a pickled file called 'dict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionary word --> id \n",
    "dictionary = pickle.load(open('dict', 'rb'))\n",
    "\n",
    "# set the maximum sequence length\n",
    "max_seq_len = 50\n",
    "\n",
    "# function to transform sentence into word id's and put them in a pytorch Variable\n",
    "# NB Assumes the sentence is already tokenised!\n",
    "def tokenise(sentence, dictionary):\n",
    "    words = sentence.split(' ')\n",
    "    l = len(words)\n",
    "    assert l <= max_seq_len, \"sentence too long\"\n",
    "    token = 0\n",
    "    ids = torch.LongTensor(l)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[token] = dictionary.word2idx[word]\n",
    "        except KeyError:\n",
    "            print( word)\n",
    "            raw_input()\n",
    "            ids[token] = dictionary.word2idx['<unk>']\n",
    "        token += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that can be used to evaluate a single sentence and print the probabilities of finishing this sentence with a word from a list of input words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-5-cf69fc0157bf>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-cf69fc0157bf>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    print sentence, '\\n'\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# load pytorch softmax function\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "def evaluate(model, dictionary, sentence, check_words):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # number of tokens (= output size)\n",
    "    ntokens = len(dictionary)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # tokenise the sentence, put in torch Variable\n",
    "    test_data = tokenise(sentence, dictionary)\n",
    "    input_data = Variable(test_data, volatile=True)\n",
    "\n",
    "    # run the model, compute probabilities by applying softmax\n",
    "    output, hidden = model(input_data, hidden)\n",
    "    output_flat = output.view(-1, ntokens)\n",
    "    logits = output[-1, :]\n",
    "    sm = softmax(logits).view(ntokens)\n",
    "    \n",
    "    # get probabilities of certain words by looking up their\n",
    "    # indices and print them\n",
    "    def get_prob(word):\n",
    "        return sm[dictionary.word2idx[word]].data[0]\n",
    "\n",
    "    print (sentence, '\\n')\n",
    "    print ('\\n'.join(\n",
    "            ['%s: %f' % (word, get_prob(word)) for word in check_words]\n",
    "            ) )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test sentence and words to check\n",
    "test_sentence = 'this is a sentence with seven'\n",
    "check_words = ['words', 'characters', 'thursday', 'days', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lm, dictionary, test_sentence, check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it yourself\n",
    "\n",
    "You can now start with your replication of Linzen's paper, for which the first step is to try different inputs with varying distances etc. to get a feeling for what the model is doing and to familiarise yourself with using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
